{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ],
      "metadata": {
        "id": "WUwAIQizHXdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "RKFYzjK0HxLW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ],
      "metadata": {
        "id": "K5xXlemwH2dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS Tagging"
      ],
      "metadata": {
        "id": "ibXOHrZZH3R-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ],
      "metadata": {
        "id": "KT3nYge7H6B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачаем данные:"
      ],
      "metadata": {
        "id": "xiZL5VRuIBcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soKx6vvEIC--",
        "outputId": "0db13e76-505e-4107-e77d-6e1b665bcd1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример размеченного предложения:"
      ],
      "metadata": {
        "id": "YsEkYwL5IMbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uP66bQkINDj",
        "outputId": "d5802e83-fd98-463e-fc26-a9ccfbd2d663"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ],
      "metadata": {
        "id": "YYwBRewZIVXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL2bKkXbIWRA",
        "outputId": "f406451d-6975-4b1e-8432-c38967acaa56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:"
      ],
      "metadata": {
        "id": "XkHKQi5dIkqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh2vghDgIl1n",
        "outputId": "aa79fc0a-4583-4f1e-9523-1198103e06af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in train = 45441. Tags = {'CONJ', 'PRT', 'VERB', 'PRON', 'X', 'NUM', 'NOUN', '.', 'DET', 'ADJ', 'ADP', 'ADV'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "DOmSThMPIub-",
        "outputId": "d20fb90d-44de-4b7d-9225-c84423022e5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAGsCAYAAAAvwW2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/OUlEQVR4nO3deVRV9d7H8c8BApzAKUGSkJxNw5s+l+iWQ5FoZlHWVTNDJS0DU8kcyhBt0PSq6b0kq1KxW6Z5n7SuFYqUWkmaKA4lTmFmcrQcOEnlxH7+aLEfj+CA/k44vF9r7VVn/777d35fhnP4uM/Zx2FZliUAAAAAgBFeFb0AAAAAALiSELIAAAAAwCBCFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQT4VvYBLWXFxsfbu3atq1arJ4XBU9HIAAAAAVBDLsvTLL78oJCREXl5nP1dFyDqLvXv3KjQ0tKKXAQAAAOAS8cMPP6hevXpnrSFknUW1atUk/fGFDAgIqODVAAAAAKgoLpdLoaGhdkY4G0LWWZS8RDAgIICQBQAAAOC83kbEhS8AAAAAwCBCFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMKnfIWrlypbp27aqQkBA5HA4tWrTIbdzhcJS5TZo0ya6pX79+qfEJEya4zbNx40bdfvvt8vf3V2hoqCZOnFhqLQsWLFDTpk3l7++vli1b6uOPP3YbtyxLycnJqlu3ripVqqTo6Ght3769vC0DAAAAwHkrd8gqKipSRESEUlNTyxwvKChw22bNmiWHw6Fu3bq51Y0bN86tbtCgQfaYy+VSx44dFRYWppycHE2aNEkpKSl6/fXX7ZpVq1apZ8+eio+P1/r16xUbG6vY2Fht3rzZrpk4caKmT5+utLQ0rV69WlWqVFFMTIx+//338rYNAAAAAOfFYVmWdcEHOxxauHChYmNjz1gTGxurX375RVlZWfa++vXra8iQIRoyZEiZx8yYMUPPPfecnE6nfH19JUkjR47UokWLlJeXJ0nq3r27ioqKtHjxYvu4W265Ra1atVJaWposy1JISIiefvppDRs2TJJUWFiooKAgpaenq0ePHufsz+VyKTAwUIWFhQoICDhnPQAAAIArU3mygY8nF7Jv3z599NFHmjNnTqmxCRMm6IUXXtD111+vhx9+WEOHDpWPzx/Lyc7OVtu2be2AJUkxMTF65ZVXdOjQIdWoUUPZ2dlKSkpymzMmJsZ++WJ+fr6cTqeio6Pt8cDAQEVGRio7O7vMkHX06FEdPXrUvu1yuS6qfwAAzmRq5jaPzDv0rsYemRcAcP48GrLmzJmjatWq6YEHHnDb/9RTT+nmm29WzZo1tWrVKo0aNUoFBQWaMmWKJMnpdCo8PNztmKCgIHusRo0acjqd9r5Ta5xOp1136nFl1Zxu/PjxGjt27AV2CwAAAAAeDlmzZs1Sr1695O/v77b/1DNQN910k3x9ffX4449r/Pjx8vPz8+SSzmrUqFFua3O5XAoNDa2w9QAAAAC4/HjsEu6ff/65tm7dqscee+yctZGRkTpx4oR27dolSQoODta+ffvcakpuBwcHn7Xm1PFTjyur5nR+fn4KCAhw2wAAAACgPDwWsmbOnKnWrVsrIiLinLW5ubny8vJSnTp1JElRUVFauXKljh8/btdkZmaqSZMmqlGjhl1z6sU0SmqioqIkSeHh4QoODnarcblcWr16tV0DAAAAAKaV++WCR44c0Y4dO+zb+fn5ys3NVc2aNXX99ddL+iPMLFiwQJMnTy51fHZ2tlavXq0OHTqoWrVqys7O1tChQ/XII4/YAerhhx/W2LFjFR8frxEjRmjz5s2aNm2apk6das8zePBgtWvXTpMnT1aXLl00b948rV271r7Mu8Ph0JAhQ/Tiiy+qUaNGCg8P1/PPP6+QkJCzXg0RAAAAAC5GuUPW2rVr1aFDB/t2yXuY4uLilJ6eLkmaN2+eLMtSz549Sx3v5+enefPmKSUlRUePHlV4eLiGDh3q9l6owMBALV26VAkJCWrdurVq166t5ORkDRgwwK659dZbNXfuXI0ePVrPPvusGjVqpEWLFqlFixZ2zfDhw1VUVKQBAwbo8OHDuu2225SRkVHqPWIAAAAAYMpFfU7WlY7PyQIAeAqXcAeAy0t5soHH3pMFAAAAAFcjQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAIEIWAAAAABhEyAIAAAAAgwhZAAAAAGAQIQsAAAAADCJkAQAAAIBBhCwAAAAAMIiQBQAAAAAGEbIAAAAAwCBCFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAIEIWAAAAABhEyAIAAAAAgwhZAAAAAGAQIQsAAAAADCJkAQAAAIBBhCwAAAAAMIiQBQAAAAAGEbIAAAAAwKByh6yVK1eqa9euCgkJkcPh0KJFi9zG+/TpI4fD4bZ16tTJrebgwYPq1auXAgICVL16dcXHx+vIkSNuNRs3btTtt98uf39/hYaGauLEiaXWsmDBAjVt2lT+/v5q2bKlPv74Y7dxy7KUnJysunXrqlKlSoqOjtb27dvL2zIAAAAAnLdyh6yioiJFREQoNTX1jDWdOnVSQUGBvb377rtu47169dI333yjzMxMLV68WCtXrtSAAQPscZfLpY4dOyosLEw5OTmaNGmSUlJS9Prrr9s1q1atUs+ePRUfH6/169crNjZWsbGx2rx5s10zceJETZ8+XWlpaVq9erWqVKmimJgY/f777+VtGwAAAADOi8OyLOuCD3Y4tHDhQsXGxtr7+vTpo8OHD5c6w1Viy5Ytat68ub7++mu1adNGkpSRkaG7775be/bsUUhIiGbMmKHnnntOTqdTvr6+kqSRI0dq0aJFysvLkyR1795dRUVFWrx4sT33LbfcolatWiktLU2WZSkkJERPP/20hg0bJkkqLCxUUFCQ0tPT1aNHj3P253K5FBgYqMLCQgUEBFzIlwgAgDJNzdzmkXmH3tXYI/MCwNWuPNnAI+/JWr58uerUqaMmTZpo4MCBOnDggD2WnZ2t6tWr2wFLkqKjo+Xl5aXVq1fbNW3btrUDliTFxMRo69atOnTokF0THR3tdr8xMTHKzs6WJOXn58vpdLrVBAYGKjIy0q453dGjR+Vyudw2AAAAACgP4yGrU6dOeuutt5SVlaVXXnlFK1asUOfOnXXy5ElJktPpVJ06ddyO8fHxUc2aNeV0Ou2aoKAgt5qS2+eqOXX81OPKqjnd+PHjFRgYaG+hoaHl7h8AAADA1c3H9ISnvgyvZcuWuummm9SgQQMtX75cd955p+m7M2rUqFFKSkqyb7tcLoIWAAAAgHLx+CXcb7jhBtWuXVs7duyQJAUHB2v//v1uNSdOnNDBgwcVHBxs1+zbt8+tpuT2uWpOHT/1uLJqTufn56eAgAC3DQAAAADKw+Mha8+ePTpw4IDq1q0rSYqKitLhw4eVk5Nj13z66acqLi5WZGSkXbNy5UodP37crsnMzFSTJk1Uo0YNuyYrK8vtvjIzMxUVFSVJCg8PV3BwsFuNy+XS6tWr7RoAAAAAMK3cIevIkSPKzc1Vbm6upD8uMJGbm6vdu3fryJEjeuaZZ/TVV19p165dysrK0n333aeGDRsqJiZGktSsWTN16tRJ/fv315o1a/Tll18qMTFRPXr0UEhIiCTp4Ycflq+vr+Lj4/XNN99o/vz5mjZtmttL+QYPHqyMjAxNnjxZeXl5SklJ0dq1a5WYmCjpjysfDhkyRC+++KI+/PBDbdq0SY8++qhCQkLcroYIAAAAACaV+z1Za9euVYcOHezbJcEnLi5OM2bM0MaNGzVnzhwdPnxYISEh6tixo1544QX5+fnZx7zzzjtKTEzUnXfeKS8vL3Xr1k3Tp0+3xwMDA7V06VIlJCSodevWql27tpKTk90+S+vWW2/V3LlzNXr0aD377LNq1KiRFi1apBYtWtg1w4cPV1FRkQYMGKDDhw/rtttuU0ZGhvz9/cvbNgAAAACcl4v6nKwrHZ+TBQDwFD4nCwAuLxX+OVkAAAAAcLUiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAIEIWAAAAABhEyAIAAAAAgwhZAAAAAGAQIQsAAAAADCJkAQAAAIBBhCwAAAAAMIiQBQAAAAAGEbIAAAAAwCBCFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAIEIWAAAAABhEyAIAAAAAgwhZAAAAAGAQIQsAAAAADCJkAQAAAIBB5Q5ZK1euVNeuXRUSEiKHw6FFixbZY8ePH9eIESPUsmVLValSRSEhIXr00Ue1d+9etznq168vh8Phtk2YMMGtZuPGjbr99tvl7++v0NBQTZw4sdRaFixYoKZNm8rf318tW7bUxx9/7DZuWZaSk5NVt25dVapUSdHR0dq+fXt5WwYAAACA81bukFVUVKSIiAilpqaWGvv111+1bt06Pf/881q3bp3ef/99bd26Vffee2+p2nHjxqmgoMDeBg0aZI+5XC517NhRYWFhysnJ0aRJk5SSkqLXX3/drlm1apV69uyp+Ph4rV+/XrGxsYqNjdXmzZvtmokTJ2r69OlKS0vT6tWrVaVKFcXExOj3338vb9sAAAAAcF4clmVZF3yww6GFCxcqNjb2jDVff/21/vrXv+r777/X9ddfL+mPM1lDhgzRkCFDyjxmxowZeu655+R0OuXr6ytJGjlypBYtWqS8vDxJUvfu3VVUVKTFixfbx91yyy1q1aqV0tLSZFmWQkJC9PTTT2vYsGGSpMLCQgUFBSk9PV09evQodb9Hjx7V0aNH7dsul0uhoaEqLCxUQEBAub42AACczdTMbR6Zd+hdjT0yLwBc7VwulwIDA88rG3j8PVmFhYVyOByqXr262/4JEyaoVq1a+stf/qJJkybpxIkT9lh2drbatm1rByxJiomJ0datW3Xo0CG7Jjo62m3OmJgYZWdnS5Ly8/PldDrdagIDAxUZGWnXnG78+PEKDAy0t9DQ0IvqHQAAAMDVx6Mh6/fff9eIESPUs2dPt7T31FNPad68efrss8/0+OOP6+WXX9bw4cPtcafTqaCgILe5Sm47nc6z1pw6fupxZdWcbtSoUSosLLS3H3744ULaBgAAAHAV8/HUxMePH9ff//53WZalGTNmuI0lJSXZ/3/TTTfJ19dXjz/+uMaPHy8/Pz9PLemc/Pz8KvT+AQAAAFz+PHImqyRgff/998rMzDznaxYjIyN14sQJ7dq1S5IUHBysffv2udWU3A4ODj5rzanjpx5XVg0AAAAAmGY8ZJUErO3bt2vZsmWqVavWOY/Jzc2Vl5eX6tSpI0mKiorSypUrdfz4cbsmMzNTTZo0UY0aNeyarKwst3kyMzMVFRUlSQoPD1dwcLBbjcvl0urVq+0aAAAAADCt3C8XPHLkiHbs2GHfzs/PV25urmrWrKm6devqwQcf1Lp167R48WKdPHnSfv9TzZo15evrq+zsbK1evVodOnRQtWrVlJ2draFDh+qRRx6xA9TDDz+ssWPHKj4+XiNGjNDmzZs1bdo0TZ061b7fwYMHq127dpo8ebK6dOmiefPmae3atfZl3h0Oh4YMGaIXX3xRjRo1Unh4uJ5//nmFhISc9WqIAAAAAHAxyn0J9+XLl6tDhw6l9sfFxSklJUXh4eFlHvfZZ5+pffv2WrdunZ588knl5eXp6NGjCg8PV+/evZWUlOT2fqiNGzcqISFBX3/9tWrXrq1BgwZpxIgRbnMuWLBAo0eP1q5du9SoUSNNnDhRd999tz1uWZbGjBmj119/XYcPH9Ztt92m1157TY0bn9/lbctzmUYAAMqDS7gDwOWlPNngoj4n60pHyAIAeAohCwAuL5fU52QBAAAAwNWEkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAIJ+KXgCAM5uauc34nEPvamx8TgAAAPw/zmQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAIEIWAAAAABhEyAIAAAAAgwhZAAAAAGAQIQsAAAAADCJkAQAAAIBBhCwAAAAAMIiQBQAAAAAGEbIAAAAAwCBCFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAIEIWAAAAABhU7pC1cuVKde3aVSEhIXI4HFq0aJHbuGVZSk5OVt26dVWpUiVFR0dr+/btbjUHDx5Ur169FBAQoOrVqys+Pl5Hjhxxq9m4caNuv/12+fv7KzQ0VBMnTiy1lgULFqhp06by9/dXy5Yt9fHHH5d7LQAAAABgUrlDVlFRkSIiIpSamlrm+MSJEzV9+nSlpaVp9erVqlKlimJiYvT777/bNb169dI333yjzMxMLV68WCtXrtSAAQPscZfLpY4dOyosLEw5OTmaNGmSUlJS9Prrr9s1q1atUs+ePRUfH6/169crNjZWsbGx2rx5c7nWAgAAAAAmOSzLsi74YIdDCxcuVGxsrKQ/zhyFhITo6aef1rBhwyRJhYWFCgoKUnp6unr06KEtW7aoefPm+vrrr9WmTRtJUkZGhu6++27t2bNHISEhmjFjhp577jk5nU75+vpKkkaOHKlFixYpLy9PktS9e3cVFRVp8eLF9npuueUWtWrVSmlpaee1lnNxuVwKDAxUYWGhAgICLvTLBFywqZnbjM859K7GxucEUH6e+P2W+B0HAE8pTzYw+p6s/Px8OZ1ORUdH2/sCAwMVGRmp7OxsSVJ2draqV69uByxJio6OlpeXl1avXm3XtG3b1g5YkhQTE6OtW7fq0KFDds2p91NSU3I/57OW0x09elQul8ttAwAAAIDyMBqynE6nJCkoKMhtf1BQkD3mdDpVp04dt3EfHx/VrFnTraasOU69jzPVnDp+rrWcbvz48QoMDLS30NDQ8+gaAAAAAP4fVxc8xahRo1RYWGhvP/zwQ0UvCQAAAMBlxmjICg4OliTt27fPbf++ffvsseDgYO3fv99t/MSJEzp48KBbTVlznHofZ6o5dfxcazmdn5+fAgIC3DYAAAAAKA+jISs8PFzBwcHKysqy97lcLq1evVpRUVGSpKioKB0+fFg5OTl2zaeffqri4mJFRkbaNStXrtTx48ftmszMTDVp0kQ1atSwa069n5Kakvs5n7UAAAAAgGnlDllHjhxRbm6ucnNzJf1xgYnc3Fzt3r1bDodDQ4YM0YsvvqgPP/xQmzZt0qOPPqqQkBD7CoTNmjVTp06d1L9/f61Zs0ZffvmlEhMT1aNHD4WEhEiSHn74Yfn6+io+Pl7ffPON5s+fr2nTpikpKclex+DBg5WRkaHJkycrLy9PKSkpWrt2rRITEyXpvNYCAAAAAKb5lPeAtWvXqkOHDvbtkuATFxen9PR0DR8+XEVFRRowYIAOHz6s2267TRkZGfL397ePeeedd5SYmKg777xTXl5e6tatm6ZPn26PBwYGaunSpUpISFDr1q1Vu3ZtJScnu32W1q233qq5c+dq9OjRevbZZ9WoUSMtWrRILVq0sGvOZy0AAAAAYNJFfU7WlY7PyUJF43OygCsXn5MFAJeXCvucLAAAAAC42hGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYFC5L+EOAABwobiqIoCrAWeyAAAAAMAgQhYAAAAAGETIAgAAAACDeE8WAAAAYBDvPQRnsgAAAADAIEIWAAAAABhEyAIAAAAAgwhZAAAAAGAQIQsAAAAADCJkAQAAAIBBhCwAAAAAMIiQBQAAAAAGEbIAAAAAwCBCFgAAAAAY5FPRCwAAALjSTc3c5pF5h97V2CPzArg4nMkCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAIEIWAAAAABhEyAIAAAAAgwhZAAAAAGAQIQsAAAAADCJkAQAAAIBBxkNW/fr15XA4Sm0JCQmSpPbt25cae+KJJ9zm2L17t7p06aLKlSurTp06euaZZ3TixAm3muXLl+vmm2+Wn5+fGjZsqPT09FJrSU1NVf369eXv76/IyEitWbPGdLsAAAAA4MZ4yPr6669VUFBgb5mZmZKkhx56yK7p37+/W83EiRPtsZMnT6pLly46duyYVq1apTlz5ig9PV3Jycl2TX5+vrp06aIOHTooNzdXQ4YM0WOPPaYlS5bYNfPnz1dSUpLGjBmjdevWKSIiQjExMdq/f7/plgEAAADAZjxkXXvttQoODra3xYsXq0GDBmrXrp1dU7lyZbeagIAAe2zp0qX69ttv9fbbb6tVq1bq3LmzXnjhBaWmpurYsWOSpLS0NIWHh2vy5Mlq1qyZEhMT9eCDD2rq1Kn2PFOmTFH//v3Vt29fNW/eXGlpaapcubJmzZplumUAAAAAsHn0PVnHjh3T22+/rX79+snhcNj733nnHdWuXVstWrTQqFGj9Ouvv9pj2dnZatmypYKCgux9MTExcrlc+uabb+ya6Ohot/uKiYlRdna2fb85OTluNV5eXoqOjrZrynL06FG5XC63DQAAAADKw8eTky9atEiHDx9Wnz597H0PP/ywwsLCFBISoo0bN2rEiBHaunWr3n//fUmS0+l0C1iS7NtOp/OsNS6XS7/99psOHTqkkydPllmTl5d3xvWOHz9eY8eOveB+AQAAAMCjIWvmzJnq3LmzQkJC7H0DBgyw/79ly5aqW7eu7rzzTu3cuVMNGjTw5HLOadSoUUpKSrJvu1wuhYaGVuCKAAAAAFxuPBayvv/+ey1btsw+Q3UmkZGRkqQdO3aoQYMGCg4OLnUVwH379kmSgoOD7f+W7Du1JiAgQJUqVZK3t7e8vb3LrCmZoyx+fn7y8/M7vwYBAAAAoAwee0/W7NmzVadOHXXp0uWsdbm5uZKkunXrSpKioqK0adMmt6sAZmZmKiAgQM2bN7drsrKy3ObJzMxUVFSUJMnX11etW7d2qykuLlZWVpZdAwAAAACe4JGQVVxcrNmzZysuLk4+Pv9/smznzp164YUXlJOTo127dunDDz/Uo48+qrZt2+qmm26SJHXs2FHNmzdX7969tWHDBi1ZskSjR49WQkKCfZbpiSee0Hfffafhw4crLy9Pr732mt577z0NHTrUvq+kpCS98cYbmjNnjrZs2aKBAweqqKhIffv29UTLAAAAACDJQy8XXLZsmXbv3q1+/fq57ff19dWyZcv06quvqqioSKGhoerWrZtGjx5t13h7e2vx4sUaOHCgoqKiVKVKFcXFxWncuHF2TXh4uD766CMNHTpU06ZNU7169fTmm28qJibGrunevbt++uknJScny+l0qlWrVsrIyCh1MQwAAAAAMMkjIatjx46yLKvU/tDQUK1YseKcx4eFhenjjz8+a0379u21fv36s9YkJiYqMTHxnPcHAAAAAKZ49HOyAAAAAOBqQ8gCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAIEIWAAAAABhEyAIAAAAAgwhZAAAAAGAQIQsAAAAADCJkAQAAAIBBhCwAAAAAMIiQBQAAAAAGEbIAAAAAwCBCFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwyKeiFwAAAIAr09TMbR6Zd+hdjT0yL2AKZ7IAAAAAwCBCFgAAAAAYZDxkpaSkyOFwuG1Nmza1x3///XclJCSoVq1aqlq1qrp166Z9+/a5zbF792516dJFlStXVp06dfTMM8/oxIkTbjXLly/XzTffLD8/PzVs2FDp6eml1pKamqr69evL399fkZGRWrNmjel2AQAAAMCNR85k3XjjjSooKLC3L774wh4bOnSo/vvf/2rBggVasWKF9u7dqwceeMAeP3nypLp06aJjx45p1apVmjNnjtLT05WcnGzX5Ofnq0uXLurQoYNyc3M1ZMgQPfbYY1qyZIldM3/+fCUlJWnMmDFat26dIiIiFBMTo/3793uiZQAAAACQ5KGQ5ePjo+DgYHurXbu2JKmwsFAzZ87UlClTdMcdd6h169aaPXu2Vq1apa+++kqStHTpUn377bd6++231apVK3Xu3FkvvPCCUlNTdezYMUlSWlqawsPDNXnyZDVr1kyJiYl68MEHNXXqVHsNU6ZMUf/+/dW3b181b95caWlpqly5smbNmuWJlgEAAABAkodC1vbt2xUSEqIbbrhBvXr10u7duyVJOTk5On78uKKjo+3apk2b6vrrr1d2drYkKTs7Wy1btlRQUJBdExMTI5fLpW+++cauOXWOkpqSOY4dO6acnBy3Gi8vL0VHR9s1ZTl69KhcLpfbBgAAAADlYTxkRUZGKj09XRkZGZoxY4by8/N1++2365dffpHT6ZSvr6+qV6/udkxQUJCcTqckyel0ugWskvGSsbPVuFwu/fbbb/r555918uTJMmtK5ijL+PHjFRgYaG+hoaEX9DUAAAAAcPUy/jlZnTt3tv//pptuUmRkpMLCwvTee++pUqVKpu/OqFGjRikpKcm+7XK5CFoAAAAAysXjl3CvXr26GjdurB07dig4OFjHjh3T4cOH3Wr27dun4OBgSVJwcHCpqw2W3D5XTUBAgCpVqqTatWvL29u7zJqSOcri5+engIAAtw0AAAAAysPjIevIkSPauXOn6tatq9atW+uaa65RVlaWPb5161bt3r1bUVFRkqSoqCht2rTJ7SqAmZmZCggIUPPmze2aU+coqSmZw9fXV61bt3arKS4uVlZWll0DAAAAAJ5gPGQNGzZMK1as0K5du7Rq1Srdf//98vb2Vs+ePRUYGKj4+HglJSXps88+U05Ojvr27auoqCjdcsstkqSOHTuqefPm6t27tzZs2KAlS5Zo9OjRSkhIkJ+fnyTpiSee0Hfffafhw4crLy9Pr732mt577z0NHTrUXkdSUpLeeOMNzZkzR1u2bNHAgQNVVFSkvn37mm4ZAAAAAGzG35O1Z88e9ezZUwcOHNC1116r2267TV999ZWuvfZaSdLUqVPl5eWlbt266ejRo4qJidFrr71mH+/t7a3Fixdr4MCBioqKUpUqVRQXF6dx48bZNeHh4froo480dOhQTZs2TfXq1dObb76pmJgYu6Z79+766aeflJycLKfTqVatWikjI6PUxTAAAAAAwCTjIWvevHlnHff391dqaqpSU1PPWBMWFqaPP/74rPO0b99e69evP2tNYmKiEhMTz1oDAAAAACZ5/D1ZAAAAAHA1IWQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAIEIWAAAAABhEyAIAAAAAgwhZAAAAAGAQIQsAAAAADCJkAQAAAIBBhCwAAAAAMIiQBQAAAAAGEbIAAAAAwCBCFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDfCp6AQAAAAAuf1Mzt3lk3qF3NfbIvJ7EmSwAAAAAMIiQBQAAAAAGEbIAAAAAwCBCFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQXwYMYBLDh9mCAAALmecyQIAAAAAgwhZAAAAAGAQIQsAAAAADCJkAQAAAIBBhCwAAAAAMIiQBQAAAAAGEbIAAAAAwCBCFgAAAAAYZDxkjR8/Xv/zP/+jatWqqU6dOoqNjdXWrVvdatq3by+Hw+G2PfHEE241u3fvVpcuXVS5cmXVqVNHzzzzjE6cOOFWs3z5ct18883y8/NTw4YNlZ6eXmo9qampql+/vvz9/RUZGak1a9aYbhkAAAAAbMZD1ooVK5SQkKCvvvpKmZmZOn78uDp27KiioiK3uv79+6ugoMDeJk6caI+dPHlSXbp00bFjx7Rq1SrNmTNH6enpSk5Otmvy8/PVpUsXdejQQbm5uRoyZIgee+wxLVmyxK6ZP3++kpKSNGbMGK1bt04RERGKiYnR/v37TbcNAAAAAJIkH9MTZmRkuN1OT09XnTp1lJOTo7Zt29r7K1eurODg4DLnWLp0qb799lstW7ZMQUFBatWqlV544QWNGDFCKSkp8vX1VVpamsLDwzV58mRJUrNmzfTFF19o6tSpiomJkSRNmTJF/fv3V9++fSVJaWlp+uijjzRr1iyNHDnSdOsAAAAA4Pn3ZBUWFkqSatas6bb/nXfeUe3atdWiRQuNGjVKv/76qz2WnZ2tli1bKigoyN4XExMjl8ulb775xq6Jjo52mzMmJkbZ2dmSpGPHjiknJ8etxsvLS9HR0XbN6Y4ePSqXy+W2AQAAAEB5GD+Tdari4mINGTJEf/vb39SiRQt7/8MPP6ywsDCFhIRo48aNGjFihLZu3ar3339fkuR0Ot0CliT7ttPpPGuNy+XSb7/9pkOHDunkyZNl1uTl5ZW53vHjx2vs2LEX1zQAAACAq5pHQ1ZCQoI2b96sL774wm3/gAED7P9v2bKl6tatqzvvvFM7d+5UgwYNPLmksxo1apSSkpLs2y6XS6GhoRW2nrJMzdxmfM6hdzU2PicAAABwtfJYyEpMTNTixYu1cuVK1atX76y1kZGRkqQdO3aoQYMGCg4OLnUVwH379kmS/T6u4OBge9+pNQEBAapUqZK8vb3l7e1dZs2Z3gvm5+cnPz+/828SAAAAAE5j/D1ZlmUpMTFRCxcu1Keffqrw8PBzHpObmytJqlu3riQpKipKmzZtcrsKYGZmpgICAtS8eXO7Jisry22ezMxMRUVFSZJ8fX3VunVrt5ri4mJlZWXZNQAAAABgmvEzWQkJCZo7d64++OADVatWzX4PVWBgoCpVqqSdO3dq7ty5uvvuu1WrVi1t3LhRQ4cOVdu2bXXTTTdJkjp27KjmzZurd+/emjhxopxOp0aPHq2EhAT7TNMTTzyhf/3rXxo+fLj69eunTz/9VO+9954++ugjey1JSUmKi4tTmzZt9Ne//lWvvvqqioqK7KsNAgAAAIBpxkPWjBkzJP3xgcOnmj17tvr06SNfX18tW7bMDjyhoaHq1q2bRo8ebdd6e3tr8eLFGjhwoKKiolSlShXFxcVp3Lhxdk14eLg++ugjDR06VNOmTVO9evX05ptv2pdvl6Tu3bvrp59+UnJyspxOp1q1aqWMjIxSF8MAAAAAAFOMhyzLss46HhoaqhUrVpxznrCwMH388cdnrWnfvr3Wr19/1prExEQlJiae8/4AAAAAwASPf04WAAAAAFxNCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwyPjVBQFPmJq5zficQ+9qbHxOAAAAgDNZAAAAAGAQIQsAAAAADCJkAQAAAIBBhCwAAAAAMIiQBQAAAAAGEbIAAAAAwCBCFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEE+Fb0AAMDVbWrmNo/MO/Suxh6ZFwCAc+FMFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAwiZAEAAACAQYQsAAAAADCIkAUAAAAABhGyAAAAAMAgQhYAAAAAGETIAgAAAACDCFkAAAAAYBAhCwAAAAAMImQBAAAAgEGELAAAAAAwiJAFAAAAAAYRsgAAAADAoKsiZKWmpqp+/fry9/dXZGSk1qxZU9FLAgAAAHCFuuJD1vz585WUlKQxY8Zo3bp1ioiIUExMjPbv31/RSwMAAABwBfKp6AV42pQpU9S/f3/17dtXkpSWlqaPPvpIs2bN0siRI91qjx49qqNHj9q3CwsLJUkul+vPW/A5/F50xPicl1J/Z0Lf5lytfUuXfu+pn+7wyLwJdzT0yLymXK3fb/o2i74vTfRtFn1XrJJ1WJZ1zlqHdT5Vl6ljx46pcuXK+s9//qPY2Fh7f1xcnA4fPqwPPvjArT4lJUVjx479k1cJAAAA4HLxww8/qF69emetuaLPZP388886efKkgoKC3PYHBQUpLy+vVP2oUaOUlJRk3y4uLtbBgwdVq1YtORwOj6/XFJfLpdDQUP3www8KCAio6OX8qa7W3umbvq8G9E3fVwP6pu+rweXat2VZ+uWXXxQSEnLO2is6ZJWXn5+f/Pz83PZVr169YhZjQEBAwGX1g2vS1do7fV9d6PvqQt9XF/q+utD35SMwMPC86q7oC1/Url1b3t7e2rdvn9v+ffv2KTg4uIJWBQAAAOBKdkWHLF9fX7Vu3VpZWVn2vuLiYmVlZSkqKqoCVwYAAADgSnXFv1wwKSlJcXFxatOmjf7617/q1VdfVVFRkX21wSuRn5+fxowZU+qlj1eDq7V3+qbvqwF90/fVgL7p+2pwNfR9RV9dsMS//vUvTZo0SU6nU61atdL06dMVGRlZ0csCAAAAcAW6KkIWAAAAAPxZruj3ZAEAAADAn42QBQAAAAAGEbIAAAAAwCBCFgAAAAAYRMi6xDmdTg0aNEg33HCD/Pz8FBoaqq5du7p99teqVat09913q0aNGvL391fLli01ZcoUnTx50m0uh8Mhf39/ff/99277Y2Nj1adPH/t2nz59FBsb68m2yqVPnz5yOBxyOBzy9fVVw4YNNW7cOJ04cULLly+3xxwOh6699lrdfffd2rRpkyS5jZW1paSkVGhvXbt2VadOncoc+/zzz+VwOLRx48Yzrv+rr76SJKWnp9v7vLy8VLduXXXv3l27d+92m7N9+/ZuxwcFBemhhx4q9TPhaRfzPT3VDz/8oH79+ikkJES+vr4KCwvT4MGDdeDAAbe6kr7nzZvntv/VV19V/fr1PdmqESdPntStt96qBx54wG1/YWGhQkND9dxzz1XQyi5Myfd/woQJbvsXLVokh8Mh6Y+f6erVq5d5vMPh0KJFiyRJu3btksPhkLe3t3788Ue3uoKCAvn4+MjhcGjXrl2m2zir8+lR+uN7O3XqVLVs2VL+/v6qUaOGOnfurC+//NLtuJSUFLVq1arU/ZT0n5ubK0n278+NN95Y6jmgevXqSk9PN9Ifzt+pj3fXXHONgoKCdNddd2nWrFkqLi626+rXr1/m4/yECROUkpJyzuezS112dra8vb3VpUsXt/0lP8MlW7Vq1XTjjTcqISFB27dvd6s92+PCpcZUv6c+t9erV099+/bV/v37/8xWyuVi+z7fv4suB4SsS9iuXbvUunVrffrpp5o0aZI2bdqkjIwMdejQQQkJCZKkhQsXql27dqpXr54+++wz5eXlafDgwXrxxRfVo0cPnX7xSIfDoeTk5Ipo56J06tRJBQUF2r59u55++mmlpKRo0qRJ9vjWrVtVUFCgJUuW6OjRo+rSpYuOHTumgoICe3v11VcVEBDgtm/YsGEV2JUUHx+vzMxM7dmzp9TY7Nmz1aZNGwUEBEiSli1b5rb2goICtW7d2q4v6e3HH3/U//7v/2rr1q166KGHSs3bv39/FRQUaO/evfrggw/0ww8/6JFHHvFck2dwod/TEt99953atGmj7du3691339WOHTuUlpZmf9j4wYMH3e7P399fo0eP1vHjx/+0Hk3x9vZWenq6MjIy9M4779j7Bw0apJo1a2rMmDEVuLoL4+/vr1deeUWHDh0yMt91112nt956y23fnDlzdN111xmZ/0Kcq0fLstSjRw+NGzdOgwcP1pYtW7R8+XKFhoaqffv2dpC8EN99912prwcqTsnj3a5du/TJJ5+oQ4cOGjx4sO655x6dOHHCrhs3blypx/lBgwZp2LBhbvvq1atXqvZSN3PmTA0aNEgrV67U3r17S42XPMdt2LBBL7/8srZs2aKIiAi3f1S+nJjqt+S5fc+ePXrjjTf0ySefqHfv3n9WG+V2sX2fz99FN910k8f7MMLCJatz587WddddZx05cqTU2KFDh6wjR45YtWrVsh544IFS4x9++KElyZo3b569T5I1bNgwy8vLy9q0aZO9/7777rPi4uLs23FxcdZ9991ntJeLUdZ67rrrLuuWW26xPvvsM0uSdejQIXuspPcNGza4HTN79mwrMDDQ8wsuh+PHj1tBQUHWCy+84Lb/l19+sapWrWrNmDHDys/PtyRZ69evP+M8ZfU2ffp0S5JVWFho72vXrp01ePBgt7p///vfVuXKlS+2lXIx8T3t1KmTVa9ePevXX391m6egoMCqXLmy9cQTT9j72rVrZ/Xt29eqVauWlZqaau+fOnWqFRYWZrQ3T5o2bZpVo0YNa+/evdaiRYusa665xsrNza3oZZVbXFycdc8991hNmza1nnnmGXv/woULrZKnpbP9vkqyFi5caFmWZf9+jB492mrUqJFbXePGja3nn3/ekmTl5+d7opUzOp8e582bZ0myPvzww1LHP/DAA1atWrXsx/8xY8ZYERERpepOf3wo+f155plnrNDQUOv333+3awMDA63Zs2ebaxLn5UzPqVlZWZYk64033rAsy7LCwsKsqVOnntec5am9FJQ8p+Xl5Vndu3e3XnrpJXvsTM9xJ0+etNq3b2+FhYVZJ06csCzr0nweL4sn+33ppZcsLy+vUs99lwITfZ/P30WXC85kXaIOHjyojIwMJSQkqEqVKqXGq1evrqVLl+rAgQNlno3p2rWrGjdurHfffddt/9/+9jfdc889GjlypMfW/meoVKmS21mNEoWFhfZLwnx9ff/sZZWbj4+PHn30UaWnp7uddVywYIFOnjypnj17XtC8+/fv18KFC+Xt7S1vb+8z1h08eFDvvffeJfHh3OX5nh48eFBLlizRk08+qUqVKrnVBwcHq1evXpo/f77b1zQgIEDPPfecxo0bp6KiIg924jmDBg1SRESEevfurQEDBig5OVkREREVvawL4u3trZdffln//Oc/y/wXy/K69957dejQIX3xxReSpC+++EKHDh1S165dL3ruC3WuHufOnavGjRuXucann35aBw4cUGZm5gXd95AhQ3TixAn985//vKDj4Xl33HGHIiIi9P7771f0UjzuvffeU9OmTdWkSRM98sgjmjVrVqlX2pzOy8tLgwcP1vfff6+cnJw/aaVmeLLfSpUqqbi42O0M6KXCRN+e+ruoIhCyLlE7duyQZVlq2rTpGWu2bdsmSWrWrFmZ402bNrVrTjV+/HhlZGTo888/N7PYP5FlWVq2bJmWLFmiO+64w95fr149Va1aVdWrV9fcuXN17733nvVrdynp16+fdu7cqRUrVtj7Zs+erW7duikwMNDed+utt6pq1apu26kKCwtVtWpVValSRUFBQfrss8/KDOmvvfaaXVerVi1t3bpVs2bN8myTZ3Eh39Pt27fLsqwz/uw3a9ZMhw4d0k8//eS2/8knn5S/v7+mTJniuYY8yOFwaMaMGcrKylJQUNBl/48l999/v1q1amXk5Y7XXHON/aQuSbNmzdIjjzyia6655qLnvhhn63Hbtm1n/RkuqbkQlStX1pgxYzR+/HgVFhZe0BzwvKZNm7q9X3DEiBGlHucvx+fq082cOdN+WXqnTp1UWFjo9px3JiWP+X/2eyovlqf63b59u9LS0tSmTRtVq1bN2HpNMdX3+f5ddKkjZF2izpX8L7RWkpo3b65HH330svoDbfHixapatar8/f3VuXNnde/e3e2iFZ9//rlycnKUnp6uxo0bKy0treIWW05NmzbVrbfeav9xuGPHDn3++eeKj493q5s/f75yc3PdtlNVq1ZNubm5Wrt2rSZPnqybb75ZL730Uqn769Wrl3Jzc7VhwwZ98cUXatiwoTp27KhffvnFYz2WxcT3tLw/+35+fho3bpz+8Y9/6Oeff77YFirErFmzVLlyZeXn5xs5A1TRXnnlFc2ZM0dbtmy56Ln69eunBQsWyOl0asGCBerXr5+BFV68s/VY3p/h8oiPj1etWrX0yiuveOw+cHEsy3K7aMUzzzxT6nG+TZs2FbjCi7d161atWbPGPgPh4+Oj7t27a+bMmec8tuT343K4sEcJ0/2W/ANq5cqV1aRJEwUFBbm9N/dSYbLv8/276FLnU9ELQNkaNWokh8OhvLy8M9Y0btxYkrRlyxbdeuutpca3bNmi5s2bl3ns2LFj1bhx44t6Y/WfqUOHDpoxY4Z8fX0VEhIiHx/3H93w8HBVr15dTZo00f79+9W9e3etXLmyglZbfvHx8Ro0aJBSU1M1e/ZsNWjQQO3atXOrCQ0NVcOGDc84h5eXlz3erFkz7dy5UwMHDtS///1vt7rAwEC7rmHDhpo5c6bq1q2r+fPn67HHHjPc2ZldzPe0YcOGcjgc2rJli+6///5Sc2/ZskU1atTQtddeW2rskUce0T/+8Q+9+OKLl8WVBU+1atUqTZ06VUuXLtWLL76o+Ph4LVu27LL6A+R0bdu2VUxMjEaNGuV2ldOAgAAVFRWpuLhYXl7//++Bhw8flqQy/zWzZcuWatq0qXr27KlmzZqpRYsWpf4xoiKcqcfGjRufMVyW7C95nA8ICCjzjNTZvh4+Pj566aWX1KdPHyUmJl5kF/CELVu2KDw83L5du3btsz7OX45mzpypEydOKCQkxN5nWZb8/Pz0r3/966zHlvwenPo1utSZ7rdatWpat26dfeXg018if6kw3ff5/F10qeNM1iWqZs2aiomJUWpqapnvHzl8+LA6duyomjVravLkyaXGP/zwQ23fvv2Mr10NDQ1VYmKinn322VKX+b0UValSRQ0bNtT1119f6o/x0yUkJGjz5s1auHDhn7S6i/f3v/9dXl5emjt3rt566y3169fvov9wHjlypObPn69169adta7kPVu//fbbRd1feV3M97RWrVq666679Nprr5Vat9Pp1DvvvKPu3buX+TX08vLS+PHjNWPGjMvqJSi//vqr+vTpo4EDB6pDhw6aOXOm1qxZc1mdtT2TCRMm6L///a+ys7PtfU2aNNGJEydKhaSSn+eS8HG6fv36afny5ZfMWawSZfXYo0cPbd++Xf/9739L1U+ePNn+OZf++Hrs2bNH+/btc6tbt26d/P39df3115d5vw899JBuvPFGjR071mA3MOHTTz/Vpk2b1K1bt4peisecOHFCb731liZPnux2dm7Dhg0KCQkp9b7xUxUXF2v69OkKDw/XX/7ylz9x1RfOE/2W/APqDTfccMkGLE/07Ym/i/50f+plNlAuO3futIKDg63mzZtb//nPf6xt27ZZ3377rTVt2jSradOmlmVZ1oIFCyxvb2+rf//+1oYNG6z8/HzrzTfftGrUqGE9+OCDVnFxsT2fTrkil2VZ1oEDB6zAwEDL39//sru6YImyrkRnWZY1fPhwq2XLlm79X+pXJYqPj7dq1KhheXt7Wz/++KO9v+SKPMuWLbMKCgrctt9++82yrDP39ve//93q0qWLfbtdu3ZW//797eNzc3Otbt26Wf7+/lZeXp7Heyxh4nu6bds2q3bt2tbtt99urVixwtq9e7f1ySefWC1atLAaNWpkHThwwD62rKsq3n777Za/v/9lc3XBp556ymrYsKFVVFRk70tLS7OqVq36p18572KV9f3v3bu35e/vb536tNSxY0crIiLCWrZsmfXdd99Zn3zyidWkSROre/fuds3pV6w6fvy49dNPP1nHjx+3LMuy1q9fX2FXFzxXj8XFxdb9999v1ahRw3rzzTet/Px8a8OGDdaAAQMsHx8ft8fr48ePWzfeeKPVoUMH68svv7R27txpLViwwKpbt641YsQIu66s35+srCzLx8fH8vHxuayuLvjPf/7TuuOOOyp6GRctLi7O6tSpk1VQUGDt2bPHysnJsV566SWratWq1j333GNfSS4sLMwaN25cqcf5U68QW+JyubrgwoULLV9fX+vw4cOlxoYPH261adOm1HPczp07rQ8++MDq0KGDValSJevTTz+1j7nUn8evtn5LmO67xJn+LrpcELIucXv37rUSEhKssLAwy9fX17ruuuuse++91/rss8/smpUrV1oxMTFWQECA5evra914443WP/7xD/uBu8TpIcuyLOvll1+2JLmFrN69e1vdunXzYFflcyF/kO/evdvy8fGx5s+fb++71B+sVq1aZUmy7r77brf9JQ9MZW3vvvuuZVln7i07O9uSZK1evdqyrD/CxqnH16hRw2rXrl2ZD26eZOp7umvXLisuLs4KCgqyrrnmGis0NNQaNGiQ9fPPP7sdW1bIKvl6Xw4ha/ny5Za3t7f1+eeflxrr2LGjdccdd7j9g8Klrqzvf35+vuXr6+sWsg4dOmQ99dRTVoMGDaxKlSpZjRo1soYPH2798ssvbsedGrJOdymFrLJ6PH78uDVp0iTrxhtvtHx9fa2AgAArJibG+uKLL0rN+eOPP1pxcXHW9ddfb1WqVMlq3ry5NWHCBOvYsWN2zZl+fzp27GhJuqxC1pgxYy6L389ziYuLsx9zfXx8rGuvvdaKjo62Zs2aZZ08edKuCwsLK/Nx/vHHHy815+USsu65555Sz2klVq9ebX80x6n9Vq5c2WrWrJn15JNPWtu3b3c7ZubMmVatWrX+jKVfENP9Xup/t5Qw3XeJM/1ddLlwWJYH33WLy1KnTp3UsGHDc76GFgAA4M8yYcIEvf3229q8eXNFLwU4J96TBduhQ4e0ePFiLV++XNHR0RW9HAAAAP36669at26dZs+ezd8nuGwQsmDr16+fnnjiCT399NO67777Kno5AAAAev311xUdHa2IiAglJydX9HKA88LLBQEAAADAIM5kAQAAAIBBhCwAAAAAMIiQBQAAAAAGEbIAAAAAwCBCFgAAAAAYRMgCAAAAAIMIWQAAAABgECELAAAAAAz6PxtA1feT26CrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ],
      "metadata": {
        "id": "q9F9rYGaI6rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.accuracy(test_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gOm0Ht7JDt3",
        "outputId": "8373ba6e-b455-4b90-8412-f75a3228b2a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавим вероятности переходов:"
      ],
      "metadata": {
        "id": "UplW0VdRJaQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.accuracy(test_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSWONKAHJbbr",
        "outputId": "56c13e3f-89c8-4eaa-dfbe-dc2f31f18662"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ],
      "metadata": {
        "id": "M6qV8fYFJkiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.accuracy(test_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIz70GWZJleO",
        "outputId": "91010d68-7fef-4720-864e-912b6b9ba983"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_with_backoff_tagger = nltk.TrigramTagger(train_data, backoff=bigram_tagger)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_with_backoff_tagger.accuracy(test_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEfX5NAPKMx4",
        "outputId": "4944234d-aa0e-42a1-d628-c3865bd09eec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of trigram tagger = 93.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ],
      "metadata": {
        "id": "ktN6tcLSKCt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "GOswthEkKJe7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "metadata": {
        "id": "s3ue3xPlKbUh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "\n",
        "        batch_indices = indices[start:end]\n",
        "\n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "\n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "\n",
        "        yield X_batch, y_batch"
      ],
      "metadata": {
        "id": "Tt4Il5C9KflU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_batches_for_pretrained(data, batch_size):\n",
        "    X = [[word for word, _ in sample] for sample in data]\n",
        "    y = [[tag for _, tag in sample] for sample in data]\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "\n",
        "        batch_indices = indices[start:end]\n",
        "\n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = []\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "\n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch.append(X[sample_ind])\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = [tag2ind.get(tag, 0) for tag in y[sample_ind]]\n",
        "\n",
        "        yield X_batch, y_batch"
      ],
      "metadata": {
        "id": "zS8WSyGT7PPl"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkzWCWYcKjEw",
        "outputId": "98e908b3-d2ab-43f0-8ae0-ed0a7cb2e5ce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ],
      "metadata": {
        "id": "TL3SkPTWKmcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.word_embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.word_embedding(inputs)\n",
        "        lstm_result, _ = self.lstm(embeddings)\n",
        "        tags = self.hidden2tag(lstm_result)\n",
        "        scores = torch.softmax(tags, dim=-1)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "KEb2bWl8Kr09"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ],
      "metadata": {
        "id": "00RF6CjHND9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)"
      ],
      "metadata": {
        "id": "5IzyPG_5NGai"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(logits, targets):\n",
        "    predictions = torch.argmax(logits, dim=2)\n",
        "    mask = ~predictions.eq(0)\n",
        "    accuracy = torch.sum((predictions == targets) * mask).item() / (targets.shape[0] * targets.shape[1])\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "zOmO1lqlNh-i"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculate_accuracy(logits, y_batch)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTJsZFkbN3h1",
        "outputId": "b638c659-22ae-489b-b704-384e8d7fa9e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.015625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "3gcWlY-mOO_C"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(logits, targets, criterion):\n",
        "    return criterion(logits.view(-1, logits.shape[-1]), targets.view(-1))"
      ],
      "metadata": {
        "id": "74biv3O-OSOA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = calculate_loss(logits, y_batch, criterion)\n",
        "print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg2pmdGcOhfP",
        "outputId": "11fe8974-b3a9-4fa0-d41f-16d38bff2da7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.562805652618408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ],
      "metadata": {
        "id": "lNkxE-PwOyOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None,\n",
        "             pretrained=False):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "\n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "\n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "\n",
        "    batches_iterator = (iterate_batches, iterate_batches_for_pretrained)[not is_train and pretrained]\n",
        "\n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(batches_iterator(data, batch_size)):\n",
        "                y_batch = LongTensor(y_batch)\n",
        "                if is_train or not pretrained:\n",
        "                    X_batch = LongTensor(X_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = calculate_loss(logits, y_batch, criterion)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                logits = logits.view(-1, logits.shape[-1])\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                y_batch = y_batch.view(-1)\n",
        "\n",
        "                if not pretrained:\n",
        "                    mask = ~preds.eq(0)\n",
        "                    cur_correct_count, cur_sum_count = torch.sum((preds == y_batch) * mask).item(), len(preds)\n",
        "                else:\n",
        "                    cur_correct_count, cur_sum_count = torch.sum(preds == y_batch).item(), len(preds)\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "\n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None, pretrained=False):\n",
        "\n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "\n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:', pretrained)"
      ],
      "metadata": {
        "id": "cf0_HdOEOy1u"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_dYLMZ4UKhG",
        "outputId": "64796423-eb2a-4f39-a61d-c3b6e40fc220"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 50] Train: Loss = 1.84029, Accuracy = 18.46%: 100%|██████████| 572/572 [00:07<00:00, 81.21it/s] \n",
            "[1 / 50]   Val: Loss = 1.78657, Accuracy = 21.99%: 100%|██████████| 101/101 [00:00<00:00, 231.65it/s]\n",
            "[2 / 50] Train: Loss = 1.76902, Accuracy = 23.95%: 100%|██████████| 572/572 [00:04<00:00, 123.67it/s]\n",
            "[2 / 50]   Val: Loss = 1.76681, Accuracy = 23.81%: 100%|██████████| 101/101 [00:00<00:00, 231.03it/s]\n",
            "[3 / 50] Train: Loss = 1.75646, Accuracy = 25.05%: 100%|██████████| 572/572 [00:04<00:00, 121.90it/s]\n",
            "[3 / 50]   Val: Loss = 1.76215, Accuracy = 23.83%: 100%|██████████| 101/101 [00:00<00:00, 227.65it/s]\n",
            "[4 / 50] Train: Loss = 1.74894, Accuracy = 25.44%: 100%|██████████| 572/572 [00:04<00:00, 124.83it/s]\n",
            "[4 / 50]   Val: Loss = 1.75154, Accuracy = 25.35%: 100%|██████████| 101/101 [00:00<00:00, 224.48it/s]\n",
            "[5 / 50] Train: Loss = 1.74057, Accuracy = 26.47%: 100%|██████████| 572/572 [00:04<00:00, 120.27it/s]\n",
            "[5 / 50]   Val: Loss = 1.74860, Accuracy = 25.45%: 100%|██████████| 101/101 [00:00<00:00, 229.55it/s]\n",
            "[6 / 50] Train: Loss = 1.73804, Accuracy = 26.49%: 100%|██████████| 572/572 [00:04<00:00, 123.22it/s]\n",
            "[6 / 50]   Val: Loss = 1.74662, Accuracy = 25.31%: 100%|██████████| 101/101 [00:00<00:00, 251.91it/s]\n",
            "[7 / 50] Train: Loss = 1.73467, Accuracy = 27.03%: 100%|██████████| 572/572 [00:04<00:00, 124.42it/s]\n",
            "[7 / 50]   Val: Loss = 1.73706, Accuracy = 27.10%: 100%|██████████| 101/101 [00:00<00:00, 225.72it/s]\n",
            "[8 / 50] Train: Loss = 1.72255, Accuracy = 28.13%: 100%|██████████| 572/572 [00:04<00:00, 119.29it/s]\n",
            "[8 / 50]   Val: Loss = 1.72818, Accuracy = 27.86%: 100%|██████████| 101/101 [00:00<00:00, 234.47it/s]\n",
            "[9 / 50] Train: Loss = 1.71566, Accuracy = 28.76%: 100%|██████████| 572/572 [00:05<00:00, 110.03it/s]\n",
            "[9 / 50]   Val: Loss = 1.72505, Accuracy = 27.63%: 100%|██████████| 101/101 [00:00<00:00, 232.47it/s]\n",
            "[10 / 50] Train: Loss = 1.71336, Accuracy = 29.02%: 100%|██████████| 572/572 [00:04<00:00, 125.51it/s]\n",
            "[10 / 50]   Val: Loss = 1.72379, Accuracy = 27.66%: 100%|██████████| 101/101 [00:00<00:00, 238.37it/s]\n",
            "[11 / 50] Train: Loss = 1.71176, Accuracy = 29.15%: 100%|██████████| 572/572 [00:04<00:00, 123.01it/s]\n",
            "[11 / 50]   Val: Loss = 1.72272, Accuracy = 27.59%: 100%|██████████| 101/101 [00:00<00:00, 239.90it/s]\n",
            "[12 / 50] Train: Loss = 1.71094, Accuracy = 29.36%: 100%|██████████| 572/572 [00:04<00:00, 121.43it/s]\n",
            "[12 / 50]   Val: Loss = 1.72243, Accuracy = 27.88%: 100%|██████████| 101/101 [00:00<00:00, 241.62it/s]\n",
            "[13 / 50] Train: Loss = 1.71002, Accuracy = 29.38%: 100%|██████████| 572/572 [00:04<00:00, 121.22it/s]\n",
            "[13 / 50]   Val: Loss = 1.72174, Accuracy = 27.82%: 100%|██████████| 101/101 [00:00<00:00, 252.14it/s]\n",
            "[14 / 50] Train: Loss = 1.70891, Accuracy = 29.44%: 100%|██████████| 572/572 [00:04<00:00, 124.29it/s]\n",
            "[14 / 50]   Val: Loss = 1.72116, Accuracy = 27.90%: 100%|██████████| 101/101 [00:00<00:00, 252.22it/s]\n",
            "[15 / 50] Train: Loss = 1.70812, Accuracy = 29.60%: 100%|██████████| 572/572 [00:04<00:00, 119.73it/s]\n",
            "[15 / 50]   Val: Loss = 1.72019, Accuracy = 27.96%: 100%|██████████| 101/101 [00:00<00:00, 204.39it/s]\n",
            "[16 / 50] Train: Loss = 1.70740, Accuracy = 29.63%: 100%|██████████| 572/572 [00:04<00:00, 122.16it/s]\n",
            "[16 / 50]   Val: Loss = 1.72022, Accuracy = 28.33%: 100%|██████████| 101/101 [00:00<00:00, 227.76it/s]\n",
            "[17 / 50] Train: Loss = 1.70681, Accuracy = 29.88%: 100%|██████████| 572/572 [00:04<00:00, 122.20it/s]\n",
            "[17 / 50]   Val: Loss = 1.71959, Accuracy = 28.16%: 100%|██████████| 101/101 [00:00<00:00, 251.34it/s]\n",
            "[18 / 50] Train: Loss = 1.70620, Accuracy = 29.75%: 100%|██████████| 572/572 [00:04<00:00, 114.60it/s]\n",
            "[18 / 50]   Val: Loss = 1.71957, Accuracy = 28.44%: 100%|██████████| 101/101 [00:00<00:00, 236.42it/s]\n",
            "[19 / 50] Train: Loss = 1.70546, Accuracy = 29.77%: 100%|██████████| 572/572 [00:04<00:00, 123.37it/s]\n",
            "[19 / 50]   Val: Loss = 1.71823, Accuracy = 28.14%: 100%|██████████| 101/101 [00:00<00:00, 236.42it/s]\n",
            "[20 / 50] Train: Loss = 1.70459, Accuracy = 29.90%: 100%|██████████| 572/572 [00:04<00:00, 122.05it/s]\n",
            "[20 / 50]   Val: Loss = 1.71789, Accuracy = 28.40%: 100%|██████████| 101/101 [00:00<00:00, 214.28it/s]\n",
            "[21 / 50] Train: Loss = 1.70434, Accuracy = 29.95%: 100%|██████████| 572/572 [00:04<00:00, 120.16it/s]\n",
            "[21 / 50]   Val: Loss = 1.71847, Accuracy = 28.54%: 100%|██████████| 101/101 [00:00<00:00, 231.77it/s]\n",
            "[22 / 50] Train: Loss = 1.70421, Accuracy = 29.98%: 100%|██████████| 572/572 [00:04<00:00, 122.48it/s]\n",
            "[22 / 50]   Val: Loss = 1.71846, Accuracy = 28.72%: 100%|██████████| 101/101 [00:00<00:00, 247.20it/s]\n",
            "[23 / 50] Train: Loss = 1.70384, Accuracy = 29.83%: 100%|██████████| 572/572 [00:04<00:00, 116.50it/s]\n",
            "[23 / 50]   Val: Loss = 1.71806, Accuracy = 28.61%: 100%|██████████| 101/101 [00:00<00:00, 214.85it/s]\n",
            "[24 / 50] Train: Loss = 1.70367, Accuracy = 29.85%: 100%|██████████| 572/572 [00:04<00:00, 123.08it/s]\n",
            "[24 / 50]   Val: Loss = 1.71752, Accuracy = 28.12%: 100%|██████████| 101/101 [00:00<00:00, 225.56it/s]\n",
            "[25 / 50] Train: Loss = 1.70361, Accuracy = 30.00%: 100%|██████████| 572/572 [00:04<00:00, 120.97it/s]\n",
            "[25 / 50]   Val: Loss = 1.71748, Accuracy = 28.27%: 100%|██████████| 101/101 [00:00<00:00, 211.93it/s]\n",
            "[26 / 50] Train: Loss = 1.70336, Accuracy = 29.90%: 100%|██████████| 572/572 [00:04<00:00, 121.71it/s]\n",
            "[26 / 50]   Val: Loss = 1.71769, Accuracy = 28.54%: 100%|██████████| 101/101 [00:00<00:00, 232.64it/s]\n",
            "[27 / 50] Train: Loss = 1.70328, Accuracy = 30.03%: 100%|██████████| 572/572 [00:04<00:00, 122.20it/s]\n",
            "[27 / 50]   Val: Loss = 1.71724, Accuracy = 28.27%: 100%|██████████| 101/101 [00:00<00:00, 231.33it/s]\n",
            "[28 / 50] Train: Loss = 1.70313, Accuracy = 30.03%: 100%|██████████| 572/572 [00:04<00:00, 121.51it/s]\n",
            "[28 / 50]   Val: Loss = 1.71678, Accuracy = 27.84%: 100%|██████████| 101/101 [00:00<00:00, 247.20it/s]\n",
            "[29 / 50] Train: Loss = 1.70295, Accuracy = 29.90%: 100%|██████████| 572/572 [00:04<00:00, 121.49it/s]\n",
            "[29 / 50]   Val: Loss = 1.71712, Accuracy = 28.00%: 100%|██████████| 101/101 [00:00<00:00, 249.13it/s]\n",
            "[30 / 50] Train: Loss = 1.70296, Accuracy = 30.04%: 100%|██████████| 572/572 [00:04<00:00, 122.01it/s]\n",
            "[30 / 50]   Val: Loss = 1.71753, Accuracy = 28.49%: 100%|██████████| 101/101 [00:00<00:00, 242.77it/s]\n",
            "[31 / 50] Train: Loss = 1.70274, Accuracy = 29.89%: 100%|██████████| 572/572 [00:04<00:00, 119.65it/s]\n",
            "[31 / 50]   Val: Loss = 1.71715, Accuracy = 28.26%: 100%|██████████| 101/101 [00:00<00:00, 211.59it/s]\n",
            "[32 / 50] Train: Loss = 1.70274, Accuracy = 30.11%: 100%|██████████| 572/572 [00:04<00:00, 122.06it/s]\n",
            "[32 / 50]   Val: Loss = 1.71711, Accuracy = 28.51%: 100%|██████████| 101/101 [00:00<00:00, 239.87it/s]\n",
            "[33 / 50] Train: Loss = 1.70262, Accuracy = 30.03%: 100%|██████████| 572/572 [00:04<00:00, 123.03it/s]\n",
            "[33 / 50]   Val: Loss = 1.71731, Accuracy = 28.87%: 100%|██████████| 101/101 [00:00<00:00, 215.87it/s]\n",
            "[34 / 50] Train: Loss = 1.70250, Accuracy = 30.01%: 100%|██████████| 572/572 [00:04<00:00, 120.68it/s]\n",
            "[34 / 50]   Val: Loss = 1.71709, Accuracy = 28.57%: 100%|██████████| 101/101 [00:00<00:00, 229.91it/s]\n",
            "[35 / 50] Train: Loss = 1.70238, Accuracy = 30.11%: 100%|██████████| 572/572 [00:04<00:00, 121.29it/s]\n",
            "[35 / 50]   Val: Loss = 1.71675, Accuracy = 28.40%: 100%|██████████| 101/101 [00:00<00:00, 229.83it/s]\n",
            "[36 / 50] Train: Loss = 1.70217, Accuracy = 29.94%: 100%|██████████| 572/572 [00:04<00:00, 121.83it/s]\n",
            "[36 / 50]   Val: Loss = 1.71685, Accuracy = 28.50%: 100%|██████████| 101/101 [00:00<00:00, 238.35it/s]\n",
            "[37 / 50] Train: Loss = 1.70228, Accuracy = 30.24%: 100%|██████████| 572/572 [00:04<00:00, 121.25it/s]\n",
            "[37 / 50]   Val: Loss = 1.71687, Accuracy = 28.45%: 100%|██████████| 101/101 [00:00<00:00, 253.87it/s]\n",
            "[38 / 50] Train: Loss = 1.70213, Accuracy = 30.21%: 100%|██████████| 572/572 [00:04<00:00, 121.97it/s]\n",
            "[38 / 50]   Val: Loss = 1.71655, Accuracy = 28.24%: 100%|██████████| 101/101 [00:00<00:00, 224.91it/s]\n",
            "[39 / 50] Train: Loss = 1.70204, Accuracy = 30.12%: 100%|██████████| 572/572 [00:04<00:00, 117.94it/s]\n",
            "[39 / 50]   Val: Loss = 1.71714, Accuracy = 29.26%: 100%|██████████| 101/101 [00:00<00:00, 225.50it/s]\n",
            "[40 / 50] Train: Loss = 1.70195, Accuracy = 30.11%: 100%|██████████| 572/572 [00:04<00:00, 119.06it/s]\n",
            "[40 / 50]   Val: Loss = 1.71667, Accuracy = 28.39%: 100%|██████████| 101/101 [00:00<00:00, 245.77it/s]\n",
            "[41 / 50] Train: Loss = 1.70188, Accuracy = 30.06%: 100%|██████████| 572/572 [00:04<00:00, 118.43it/s]\n",
            "[41 / 50]   Val: Loss = 1.71676, Accuracy = 28.55%: 100%|██████████| 101/101 [00:00<00:00, 231.10it/s]\n",
            "[42 / 50] Train: Loss = 1.70186, Accuracy = 30.19%: 100%|██████████| 572/572 [00:04<00:00, 120.63it/s]\n",
            "[42 / 50]   Val: Loss = 1.71632, Accuracy = 28.41%: 100%|██████████| 101/101 [00:00<00:00, 227.75it/s]\n",
            "[43 / 50] Train: Loss = 1.70177, Accuracy = 30.11%: 100%|██████████| 572/572 [00:04<00:00, 120.68it/s]\n",
            "[43 / 50]   Val: Loss = 1.71635, Accuracy = 28.55%: 100%|██████████| 101/101 [00:00<00:00, 216.56it/s]\n",
            "[44 / 50] Train: Loss = 1.70160, Accuracy = 30.16%: 100%|██████████| 572/572 [00:04<00:00, 118.61it/s]\n",
            "[44 / 50]   Val: Loss = 1.71575, Accuracy = 27.97%: 100%|██████████| 101/101 [00:00<00:00, 214.54it/s]\n",
            "[45 / 50] Train: Loss = 1.70157, Accuracy = 30.17%: 100%|██████████| 572/572 [00:04<00:00, 116.76it/s]\n",
            "[45 / 50]   Val: Loss = 1.71597, Accuracy = 28.14%: 100%|██████████| 101/101 [00:00<00:00, 234.92it/s]\n",
            "[46 / 50] Train: Loss = 1.70143, Accuracy = 30.08%: 100%|██████████| 572/572 [00:04<00:00, 120.25it/s]\n",
            "[46 / 50]   Val: Loss = 1.71624, Accuracy = 28.68%: 100%|██████████| 101/101 [00:00<00:00, 236.94it/s]\n",
            "[47 / 50] Train: Loss = 1.70138, Accuracy = 30.22%: 100%|██████████| 572/572 [00:04<00:00, 120.58it/s]\n",
            "[47 / 50]   Val: Loss = 1.71578, Accuracy = 28.41%: 100%|██████████| 101/101 [00:00<00:00, 225.74it/s]\n",
            "[48 / 50] Train: Loss = 1.70119, Accuracy = 30.10%: 100%|██████████| 572/572 [00:04<00:00, 120.43it/s]\n",
            "[48 / 50]   Val: Loss = 1.71600, Accuracy = 28.44%: 100%|██████████| 101/101 [00:00<00:00, 221.27it/s]\n",
            "[49 / 50] Train: Loss = 1.70114, Accuracy = 30.08%: 100%|██████████| 572/572 [00:04<00:00, 121.86it/s]\n",
            "[49 / 50]   Val: Loss = 1.71616, Accuracy = 28.52%: 100%|██████████| 101/101 [00:00<00:00, 230.63it/s]\n",
            "[50 / 50] Train: Loss = 1.70107, Accuracy = 30.10%: 100%|██████████| 572/572 [00:04<00:00, 116.92it/s]\n",
            "[50 / 50]   Val: Loss = 1.71589, Accuracy = 28.30%: 100%|██████████| 101/101 [00:00<00:00, 229.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ],
      "metadata": {
        "id": "VLWmaCrm0rWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FNes8clN0sPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ],
      "metadata": {
        "id": "3N2vCOkv015J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for X_batch_test, y_batch_test in iterate_batches((X_test, y_test), 64):\n",
        "        X_batch_test, y_batch_test = torch.cuda.LongTensor(X_batch_test), torch.cuda.LongTensor(y_batch_test)\n",
        "        logits = model(X_batch_test)\n",
        "        predictions = torch.argmax(logits, dim=2)\n",
        "        mask = ~predictions.eq(0)\n",
        "        correct += torch.sum((predictions == y_batch_test) * mask).item()\n",
        "        total += len(predictions.view(-1))\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "TPeVQ8HYnaPV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_accuracy = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "print(\"Base LSTM test accuracy is {:.2%}\".format(lstm_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrcWVuO5022M",
        "outputId": "8c5e4ddb-f6de-4b34-f7f7-e044bcea5775"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base LSTM test accuracy is 29.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ],
      "metadata": {
        "id": "wuwjfXXE2aXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.word_embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.word_embedding(inputs)\n",
        "        embeddings_with_dropout = self.dropout(embeddings)\n",
        "        lstm_result, _ = self.lstm(embeddings_with_dropout)\n",
        "        tags = self.hidden2tag(lstm_result)\n",
        "        scores = torch.softmax(tags, dim=-1)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "RS7h_QdN2dZq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bi_model = BiLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(bi_model.parameters())\n",
        "\n",
        "fit(bi_model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQI699qJ3N7w",
        "outputId": "2ce17178-553d-4f88-9590-4f648a1a77d5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 50] Train: Loss = 1.83124, Accuracy = 18.73%: 100%|██████████| 572/572 [00:05<00:00, 100.58it/s]\n",
            "[1 / 50]   Val: Loss = 1.78725, Accuracy = 21.80%: 100%|██████████| 101/101 [00:00<00:00, 200.31it/s]\n",
            "[2 / 50] Train: Loss = 1.77349, Accuracy = 23.16%: 100%|██████████| 572/572 [00:05<00:00, 97.68it/s] \n",
            "[2 / 50]   Val: Loss = 1.77064, Accuracy = 23.42%: 100%|██████████| 101/101 [00:00<00:00, 200.60it/s]\n",
            "[3 / 50] Train: Loss = 1.76172, Accuracy = 24.52%: 100%|██████████| 572/572 [00:05<00:00, 103.22it/s]\n",
            "[3 / 50]   Val: Loss = 1.75468, Accuracy = 24.60%: 100%|██████████| 101/101 [00:00<00:00, 202.94it/s]\n",
            "[4 / 50] Train: Loss = 1.75230, Accuracy = 25.28%: 100%|██████████| 572/572 [00:05<00:00, 97.96it/s] \n",
            "[4 / 50]   Val: Loss = 1.75336, Accuracy = 25.15%: 100%|██████████| 101/101 [00:00<00:00, 197.92it/s]\n",
            "[5 / 50] Train: Loss = 1.75043, Accuracy = 25.63%: 100%|██████████| 572/572 [00:05<00:00, 103.03it/s]\n",
            "[5 / 50]   Val: Loss = 1.75126, Accuracy = 24.86%: 100%|██████████| 101/101 [00:00<00:00, 195.63it/s]\n",
            "[6 / 50] Train: Loss = 1.74810, Accuracy = 25.85%: 100%|██████████| 572/572 [00:05<00:00, 98.80it/s] \n",
            "[6 / 50]   Val: Loss = 1.74239, Accuracy = 26.15%: 100%|██████████| 101/101 [00:00<00:00, 200.06it/s]\n",
            "[7 / 50] Train: Loss = 1.73817, Accuracy = 26.61%: 100%|██████████| 572/572 [00:05<00:00, 102.50it/s]\n",
            "[7 / 50]   Val: Loss = 1.73892, Accuracy = 26.74%: 100%|██████████| 101/101 [00:00<00:00, 207.29it/s]\n",
            "[8 / 50] Train: Loss = 1.72912, Accuracy = 27.71%: 100%|██████████| 572/572 [00:05<00:00, 99.58it/s] \n",
            "[8 / 50]   Val: Loss = 1.72317, Accuracy = 27.57%: 100%|██████████| 101/101 [00:00<00:00, 198.88it/s]\n",
            "[9 / 50] Train: Loss = 1.72010, Accuracy = 28.39%: 100%|██████████| 572/572 [00:05<00:00, 102.74it/s]\n",
            "[9 / 50]   Val: Loss = 1.71959, Accuracy = 28.08%: 100%|██████████| 101/101 [00:00<00:00, 200.60it/s]\n",
            "[10 / 50] Train: Loss = 1.71731, Accuracy = 28.69%: 100%|██████████| 572/572 [00:05<00:00, 100.35it/s]\n",
            "[10 / 50]   Val: Loss = 1.71826, Accuracy = 28.38%: 100%|██████████| 101/101 [00:00<00:00, 210.64it/s]\n",
            "[11 / 50] Train: Loss = 1.71554, Accuracy = 28.73%: 100%|██████████| 572/572 [00:05<00:00, 104.99it/s]\n",
            "[11 / 50]   Val: Loss = 1.71747, Accuracy = 28.18%: 100%|██████████| 101/101 [00:00<00:00, 199.77it/s]\n",
            "[12 / 50] Train: Loss = 1.71406, Accuracy = 28.72%: 100%|██████████| 572/572 [00:05<00:00, 102.56it/s]\n",
            "[12 / 50]   Val: Loss = 1.71698, Accuracy = 28.41%: 100%|██████████| 101/101 [00:00<00:00, 185.31it/s]\n",
            "[13 / 50] Train: Loss = 1.71350, Accuracy = 29.23%: 100%|██████████| 572/572 [00:05<00:00, 103.97it/s]\n",
            "[13 / 50]   Val: Loss = 1.71616, Accuracy = 28.31%: 100%|██████████| 101/101 [00:00<00:00, 205.70it/s]\n",
            "[14 / 50] Train: Loss = 1.71271, Accuracy = 29.30%: 100%|██████████| 572/572 [00:05<00:00, 101.38it/s]\n",
            "[14 / 50]   Val: Loss = 1.71640, Accuracy = 28.15%: 100%|██████████| 101/101 [00:00<00:00, 199.58it/s]\n",
            "[15 / 50] Train: Loss = 1.71033, Accuracy = 29.38%: 100%|██████████| 572/572 [00:05<00:00, 103.47it/s]\n",
            "[15 / 50]   Val: Loss = 1.70990, Accuracy = 29.69%: 100%|██████████| 101/101 [00:00<00:00, 201.70it/s]\n",
            "[16 / 50] Train: Loss = 1.70466, Accuracy = 29.81%: 100%|██████████| 572/572 [00:05<00:00, 102.25it/s]\n",
            "[16 / 50]   Val: Loss = 1.70890, Accuracy = 29.04%: 100%|██████████| 101/101 [00:00<00:00, 194.76it/s]\n",
            "[17 / 50] Train: Loss = 1.70386, Accuracy = 30.10%: 100%|██████████| 572/572 [00:05<00:00, 100.99it/s]\n",
            "[17 / 50]   Val: Loss = 1.70836, Accuracy = 29.46%: 100%|██████████| 101/101 [00:00<00:00, 206.53it/s]\n",
            "[18 / 50] Train: Loss = 1.70329, Accuracy = 30.06%: 100%|██████████| 572/572 [00:05<00:00, 104.55it/s]\n",
            "[18 / 50]   Val: Loss = 1.70797, Accuracy = 29.32%: 100%|██████████| 101/101 [00:00<00:00, 198.13it/s]\n",
            "[19 / 50] Train: Loss = 1.70275, Accuracy = 30.07%: 100%|██████████| 572/572 [00:05<00:00, 100.31it/s]\n",
            "[19 / 50]   Val: Loss = 1.70844, Accuracy = 29.22%: 100%|██████████| 101/101 [00:00<00:00, 209.78it/s]\n",
            "[20 / 50] Train: Loss = 1.70225, Accuracy = 30.13%: 100%|██████████| 572/572 [00:05<00:00, 103.18it/s]\n",
            "[20 / 50]   Val: Loss = 1.70696, Accuracy = 29.18%: 100%|██████████| 101/101 [00:00<00:00, 207.00it/s]\n",
            "[21 / 50] Train: Loss = 1.70102, Accuracy = 30.17%: 100%|██████████| 572/572 [00:05<00:00, 100.05it/s]\n",
            "[21 / 50]   Val: Loss = 1.70597, Accuracy = 29.39%: 100%|██████████| 101/101 [00:00<00:00, 193.78it/s]\n",
            "[22 / 50] Train: Loss = 1.69973, Accuracy = 30.17%: 100%|██████████| 572/572 [00:05<00:00, 102.99it/s]\n",
            "[22 / 50]   Val: Loss = 1.70550, Accuracy = 29.70%: 100%|██████████| 101/101 [00:00<00:00, 200.57it/s]\n",
            "[23 / 50] Train: Loss = 1.69926, Accuracy = 30.44%: 100%|██████████| 572/572 [00:05<00:00, 100.76it/s]\n",
            "[23 / 50]   Val: Loss = 1.70531, Accuracy = 29.30%: 100%|██████████| 101/101 [00:00<00:00, 206.72it/s]\n",
            "[24 / 50] Train: Loss = 1.69866, Accuracy = 30.58%: 100%|██████████| 572/572 [00:05<00:00, 102.95it/s]\n",
            "[24 / 50]   Val: Loss = 1.70459, Accuracy = 30.04%: 100%|██████████| 101/101 [00:00<00:00, 198.86it/s]\n",
            "[25 / 50] Train: Loss = 1.69814, Accuracy = 30.55%: 100%|██████████| 572/572 [00:05<00:00, 99.54it/s]\n",
            "[25 / 50]   Val: Loss = 1.70399, Accuracy = 29.81%: 100%|██████████| 101/101 [00:00<00:00, 188.60it/s]\n",
            "[26 / 50] Train: Loss = 1.69780, Accuracy = 30.72%: 100%|██████████| 572/572 [00:05<00:00, 102.74it/s]\n",
            "[26 / 50]   Val: Loss = 1.70422, Accuracy = 29.70%: 100%|██████████| 101/101 [00:00<00:00, 208.15it/s]\n",
            "[27 / 50] Train: Loss = 1.69748, Accuracy = 30.49%: 100%|██████████| 572/572 [00:05<00:00, 100.09it/s]\n",
            "[27 / 50]   Val: Loss = 1.70365, Accuracy = 29.70%: 100%|██████████| 101/101 [00:00<00:00, 190.06it/s]\n",
            "[28 / 50] Train: Loss = 1.69719, Accuracy = 30.63%: 100%|██████████| 572/572 [00:05<00:00, 101.51it/s]\n",
            "[28 / 50]   Val: Loss = 1.70419, Accuracy = 30.63%: 100%|██████████| 101/101 [00:00<00:00, 202.47it/s]\n",
            "[29 / 50] Train: Loss = 1.69703, Accuracy = 30.76%: 100%|██████████| 572/572 [00:05<00:00, 99.84it/s] \n",
            "[29 / 50]   Val: Loss = 1.70311, Accuracy = 29.22%: 100%|██████████| 101/101 [00:00<00:00, 176.35it/s]\n",
            "[30 / 50] Train: Loss = 1.69679, Accuracy = 30.75%: 100%|██████████| 572/572 [00:05<00:00, 99.76it/s] \n",
            "[30 / 50]   Val: Loss = 1.70355, Accuracy = 29.69%: 100%|██████████| 101/101 [00:00<00:00, 196.77it/s]\n",
            "[31 / 50] Train: Loss = 1.69659, Accuracy = 30.72%: 100%|██████████| 572/572 [00:05<00:00, 99.86it/s] \n",
            "[31 / 50]   Val: Loss = 1.70375, Accuracy = 29.79%: 100%|██████████| 101/101 [00:00<00:00, 189.66it/s]\n",
            "[32 / 50] Train: Loss = 1.69637, Accuracy = 30.50%: 100%|██████████| 572/572 [00:05<00:00, 98.78it/s] \n",
            "[32 / 50]   Val: Loss = 1.70377, Accuracy = 30.01%: 100%|██████████| 101/101 [00:00<00:00, 200.15it/s]\n",
            "[33 / 50] Train: Loss = 1.69624, Accuracy = 30.63%: 100%|██████████| 572/572 [00:05<00:00, 100.82it/s]\n",
            "[33 / 50]   Val: Loss = 1.70328, Accuracy = 29.59%: 100%|██████████| 101/101 [00:00<00:00, 190.57it/s]\n",
            "[34 / 50] Train: Loss = 1.69609, Accuracy = 30.66%: 100%|██████████| 572/572 [00:05<00:00, 100.85it/s]\n",
            "[34 / 50]   Val: Loss = 1.70265, Accuracy = 29.65%: 100%|██████████| 101/101 [00:00<00:00, 199.78it/s]\n",
            "[35 / 50] Train: Loss = 1.69596, Accuracy = 30.64%: 100%|██████████| 572/572 [00:05<00:00, 101.28it/s]\n",
            "[35 / 50]   Val: Loss = 1.70293, Accuracy = 29.32%: 100%|██████████| 101/101 [00:00<00:00, 189.61it/s]\n",
            "[36 / 50] Train: Loss = 1.69585, Accuracy = 30.77%: 100%|██████████| 572/572 [00:05<00:00, 101.84it/s]\n",
            "[36 / 50]   Val: Loss = 1.70341, Accuracy = 29.43%: 100%|██████████| 101/101 [00:00<00:00, 203.70it/s]\n",
            "[37 / 50] Train: Loss = 1.69565, Accuracy = 30.61%: 100%|██████████| 572/572 [00:05<00:00, 101.53it/s]\n",
            "[37 / 50]   Val: Loss = 1.70328, Accuracy = 29.35%: 100%|██████████| 101/101 [00:00<00:00, 204.70it/s]\n",
            "[38 / 50] Train: Loss = 1.69535, Accuracy = 30.86%: 100%|██████████| 572/572 [00:05<00:00, 97.89it/s]\n",
            "[38 / 50]   Val: Loss = 1.70341, Accuracy = 29.72%: 100%|██████████| 101/101 [00:00<00:00, 193.88it/s]\n",
            "[39 / 50] Train: Loss = 1.69524, Accuracy = 30.73%: 100%|██████████| 572/572 [00:05<00:00, 101.18it/s]\n",
            "[39 / 50]   Val: Loss = 1.70272, Accuracy = 29.20%: 100%|██████████| 101/101 [00:00<00:00, 188.96it/s]\n",
            "[40 / 50] Train: Loss = 1.69515, Accuracy = 30.77%: 100%|██████████| 572/572 [00:05<00:00, 96.70it/s]\n",
            "[40 / 50]   Val: Loss = 1.70180, Accuracy = 29.65%: 100%|██████████| 101/101 [00:00<00:00, 189.02it/s]\n",
            "[41 / 50] Train: Loss = 1.69508, Accuracy = 30.89%: 100%|██████████| 572/572 [00:05<00:00, 101.55it/s]\n",
            "[41 / 50]   Val: Loss = 1.70250, Accuracy = 29.38%: 100%|██████████| 101/101 [00:00<00:00, 190.77it/s]\n",
            "[42 / 50] Train: Loss = 1.69492, Accuracy = 30.82%: 100%|██████████| 572/572 [00:05<00:00, 99.66it/s]\n",
            "[42 / 50]   Val: Loss = 1.70220, Accuracy = 29.57%: 100%|██████████| 101/101 [00:00<00:00, 183.58it/s]\n",
            "[43 / 50] Train: Loss = 1.69482, Accuracy = 30.70%: 100%|██████████| 572/572 [00:05<00:00, 100.07it/s]\n",
            "[43 / 50]   Val: Loss = 1.70251, Accuracy = 29.57%: 100%|██████████| 101/101 [00:00<00:00, 212.09it/s]\n",
            "[44 / 50] Train: Loss = 1.69478, Accuracy = 30.82%: 100%|██████████| 572/572 [00:05<00:00, 99.98it/s]\n",
            "[44 / 50]   Val: Loss = 1.70279, Accuracy = 29.39%: 100%|██████████| 101/101 [00:00<00:00, 181.91it/s]\n",
            "[45 / 50] Train: Loss = 1.69469, Accuracy = 30.89%: 100%|██████████| 572/572 [00:05<00:00, 98.11it/s] \n",
            "[45 / 50]   Val: Loss = 1.70332, Accuracy = 30.26%: 100%|██████████| 101/101 [00:00<00:00, 207.02it/s]\n",
            "[46 / 50] Train: Loss = 1.69465, Accuracy = 30.82%: 100%|██████████| 572/572 [00:05<00:00, 100.12it/s]\n",
            "[46 / 50]   Val: Loss = 1.70220, Accuracy = 29.76%: 100%|██████████| 101/101 [00:00<00:00, 175.18it/s]\n",
            "[47 / 50] Train: Loss = 1.69451, Accuracy = 30.79%: 100%|██████████| 572/572 [00:05<00:00, 98.04it/s]\n",
            "[47 / 50]   Val: Loss = 1.70262, Accuracy = 30.44%: 100%|██████████| 101/101 [00:00<00:00, 198.98it/s]\n",
            "[48 / 50] Train: Loss = 1.69446, Accuracy = 30.88%: 100%|██████████| 572/572 [00:05<00:00, 98.36it/s] \n",
            "[48 / 50]   Val: Loss = 1.70306, Accuracy = 29.50%: 100%|██████████| 101/101 [00:00<00:00, 202.27it/s]\n",
            "[49 / 50] Train: Loss = 1.69442, Accuracy = 30.74%: 100%|██████████| 572/572 [00:05<00:00, 99.81it/s]\n",
            "[49 / 50]   Val: Loss = 1.70351, Accuracy = 30.07%: 100%|██████████| 101/101 [00:00<00:00, 203.82it/s]\n",
            "[50 / 50] Train: Loss = 1.69437, Accuracy = 30.97%: 100%|██████████| 572/572 [00:05<00:00, 99.34it/s]\n",
            "[50 / 50]   Val: Loss = 1.70261, Accuracy = 29.52%: 100%|██████████| 101/101 [00:00<00:00, 199.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bi_lstm_accuracy = evaluate_model(bi_model, X_test, y_test)\n",
        "\n",
        "print(\"Bidirectional LSTM test accuracy is {:.2%}\".format(bi_lstm_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YILdAiVp6XWy",
        "outputId": "71b7eb13-59da-4130-a1c0-fd9ca181137b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bidirectional LSTM test accuracy is 30.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ],
      "metadata": {
        "id": "FWWVZy8g6mLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-apFx3Ly6nGR",
        "outputId": "1f609799-c34b-471b-8530-a5bcc07e8fb1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ],
      "metadata": {
        "id": "WQIIieIR7DQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.key_to_index:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "\n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDiDO1UM7EH8",
        "outputId": "275a9198-6718-44da-bab9-1b57fe330f46"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ],
      "metadata": {
        "id": "gP52LAWV7gAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embeddings), freeze=True)\n",
        "        self.embedding_dim = self.word_embedding.weight.shape[-1]\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.training:\n",
        "            embeddings = self.word_embedding(inputs)\n",
        "        else:\n",
        "            batch_size = len(inputs)\n",
        "            max_sent_len = max(len(sent) for sent in inputs)\n",
        "            embeddings = self.word_embedding.weight.new_zeros((max_sent_len, batch_size, self.embedding_dim))\n",
        "            for i, sent in enumerate(inputs):\n",
        "                for j, word in enumerate(sent):\n",
        "                    if word in w2v_model.key_to_index:\n",
        "                        embeddings[j, i] = torch.from_numpy(w2v_model.get_vector(word))\n",
        "        embeddings_with_dropout = self.dropout(embeddings)\n",
        "        lstm_result, _ = self.lstm(embeddings_with_dropout)\n",
        "        outputs = self.hidden2tag(lstm_result)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "0EW_cUCd7g2G"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(new_model.parameters())\n",
        "\n",
        "fit(new_model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=val_data, val_batch_size=64, pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MfGLBVj9CS9",
        "outputId": "aca9940a-969e-4976-a0eb-42d2cb1529bf"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 50] Train: Loss = 0.46160, Accuracy = 27.10%: 100%|██████████| 572/572 [00:04<00:00, 123.97it/s]\n",
            "[1 / 50]   Val: Loss = 69.17860, Accuracy = 25.46%: : 101it [00:02, 34.86it/s]\n",
            "[2 / 50] Train: Loss = 0.11139, Accuracy = 30.21%: 100%|██████████| 572/572 [00:04<00:00, 123.13it/s]\n",
            "[2 / 50]   Val: Loss = 71.25201, Accuracy = 24.97%: : 101it [00:03, 30.49it/s]\n",
            "[3 / 50] Train: Loss = 0.07795, Accuracy = 30.54%: 100%|██████████| 572/572 [00:04<00:00, 126.09it/s]\n",
            "[3 / 50]   Val: Loss = 69.81128, Accuracy = 25.40%: : 101it [00:02, 34.73it/s]\n",
            "[4 / 50] Train: Loss = 0.06108, Accuracy = 30.74%: 100%|██████████| 572/572 [00:04<00:00, 123.58it/s]\n",
            "[4 / 50]   Val: Loss = 75.09033, Accuracy = 25.68%: : 101it [00:03, 28.34it/s]\n",
            "[5 / 50] Train: Loss = 0.04983, Accuracy = 30.94%: 100%|██████████| 572/572 [00:04<00:00, 124.76it/s]\n",
            "[5 / 50]   Val: Loss = 73.89227, Accuracy = 25.21%: : 101it [00:03, 29.12it/s]\n",
            "[6 / 50] Train: Loss = 0.04239, Accuracy = 30.94%: 100%|██████████| 572/572 [00:04<00:00, 127.67it/s]\n",
            "[6 / 50]   Val: Loss = 79.84806, Accuracy = 24.89%: : 101it [00:05, 18.60it/s]\n",
            "[7 / 50] Train: Loss = 0.03733, Accuracy = 31.03%: 100%|██████████| 572/572 [00:04<00:00, 118.50it/s]\n",
            "[7 / 50]   Val: Loss = 77.69867, Accuracy = 24.89%: : 101it [00:02, 34.11it/s]\n",
            "[8 / 50] Train: Loss = 0.03289, Accuracy = 31.16%: 100%|██████████| 572/572 [00:04<00:00, 128.58it/s]\n",
            "[8 / 50]   Val: Loss = 83.11295, Accuracy = 24.43%: : 101it [00:03, 29.49it/s]\n",
            "[9 / 50] Train: Loss = 0.02974, Accuracy = 31.11%: 100%|██████████| 572/572 [00:04<00:00, 127.19it/s]\n",
            "[9 / 50]   Val: Loss = 81.63046, Accuracy = 24.78%: : 101it [00:02, 34.36it/s]\n",
            "[10 / 50] Train: Loss = 0.02662, Accuracy = 31.01%: 100%|██████████| 572/572 [00:04<00:00, 122.29it/s]\n",
            "[10 / 50]   Val: Loss = 85.89810, Accuracy = 24.53%: : 101it [00:03, 31.44it/s]\n",
            "[11 / 50] Train: Loss = 0.02407, Accuracy = 31.27%: 100%|██████████| 572/572 [00:04<00:00, 125.57it/s]\n",
            "[11 / 50]   Val: Loss = 94.40182, Accuracy = 24.38%: : 101it [00:02, 34.58it/s]\n",
            "[12 / 50] Train: Loss = 0.02201, Accuracy = 30.98%: 100%|██████████| 572/572 [00:04<00:00, 117.56it/s]\n",
            "[12 / 50]   Val: Loss = 93.53524, Accuracy = 25.07%: : 101it [00:03, 30.75it/s]\n",
            "[13 / 50] Train: Loss = 0.02001, Accuracy = 31.12%: 100%|██████████| 572/572 [00:04<00:00, 124.76it/s]\n",
            "[13 / 50]   Val: Loss = 95.14977, Accuracy = 24.50%: : 101it [00:03, 28.45it/s]\n",
            "[14 / 50] Train: Loss = 0.01835, Accuracy = 31.26%: 100%|██████████| 572/572 [00:04<00:00, 121.80it/s]\n",
            "[14 / 50]   Val: Loss = 97.73210, Accuracy = 24.77%: : 101it [00:02, 33.95it/s]\n",
            "[15 / 50] Train: Loss = 0.01687, Accuracy = 31.18%: 100%|██████████| 572/572 [00:04<00:00, 119.58it/s]\n",
            "[15 / 50]   Val: Loss = 111.80816, Accuracy = 23.78%: : 101it [00:02, 34.50it/s]\n",
            "[16 / 50] Train: Loss = 0.01547, Accuracy = 31.04%: 100%|██████████| 572/572 [00:04<00:00, 125.78it/s]\n",
            "[16 / 50]   Val: Loss = 112.68476, Accuracy = 24.25%: : 101it [00:03, 32.78it/s]\n",
            "[17 / 50] Train: Loss = 0.01384, Accuracy = 31.28%: 100%|██████████| 572/572 [00:04<00:00, 120.44it/s]\n",
            "[17 / 50]   Val: Loss = 106.90254, Accuracy = 24.20%: : 101it [00:02, 34.78it/s]\n",
            "[18 / 50] Train: Loss = 0.01291, Accuracy = 31.27%: 100%|██████████| 572/572 [00:04<00:00, 124.51it/s]\n",
            "[18 / 50]   Val: Loss = 115.13394, Accuracy = 24.40%: : 101it [00:03, 28.87it/s]\n",
            "[19 / 50] Train: Loss = 0.01174, Accuracy = 31.39%: 100%|██████████| 572/572 [00:04<00:00, 123.89it/s]\n",
            "[19 / 50]   Val: Loss = 113.22772, Accuracy = 24.14%: : 101it [00:02, 34.34it/s]\n",
            "[20 / 50] Train: Loss = 0.01081, Accuracy = 31.32%: 100%|██████████| 572/572 [00:05<00:00, 114.36it/s]\n",
            "[20 / 50]   Val: Loss = 127.10286, Accuracy = 23.60%: : 101it [00:02, 33.88it/s]\n",
            "[21 / 50] Train: Loss = 0.00999, Accuracy = 31.28%: 100%|██████████| 572/572 [00:04<00:00, 124.61it/s]\n",
            "[21 / 50]   Val: Loss = 128.21042, Accuracy = 24.00%: : 101it [00:03, 31.27it/s]\n",
            "[22 / 50] Train: Loss = 0.00926, Accuracy = 31.28%: 100%|██████████| 572/572 [00:04<00:00, 119.94it/s]\n",
            "[22 / 50]   Val: Loss = 123.22216, Accuracy = 24.32%: : 101it [00:02, 33.96it/s]\n",
            "[23 / 50] Train: Loss = 0.00863, Accuracy = 31.00%: 100%|██████████| 572/572 [00:04<00:00, 124.24it/s]\n",
            "[23 / 50]   Val: Loss = 127.00882, Accuracy = 23.98%: : 101it [00:03, 29.87it/s]\n",
            "[24 / 50] Train: Loss = 0.00798, Accuracy = 31.37%: 100%|██████████| 572/572 [00:04<00:00, 125.26it/s]\n",
            "[24 / 50]   Val: Loss = 134.00773, Accuracy = 23.77%: : 101it [00:02, 34.40it/s]\n",
            "[25 / 50] Train: Loss = 0.00754, Accuracy = 31.33%: 100%|██████████| 572/572 [00:04<00:00, 121.34it/s]\n",
            "[25 / 50]   Val: Loss = 146.11154, Accuracy = 23.36%: : 101it [00:03, 33.38it/s]\n",
            "[26 / 50] Train: Loss = 0.00706, Accuracy = 31.46%: 100%|██████████| 572/572 [00:04<00:00, 123.32it/s]\n",
            "[26 / 50]   Val: Loss = 145.06056, Accuracy = 23.12%: : 101it [00:03, 31.45it/s]\n",
            "[27 / 50] Train: Loss = 0.00648, Accuracy = 31.26%: 100%|██████████| 572/572 [00:04<00:00, 123.48it/s]\n",
            "[27 / 50]   Val: Loss = 145.49679, Accuracy = 23.52%: : 101it [00:02, 34.48it/s]\n",
            "[28 / 50] Train: Loss = 0.00612, Accuracy = 31.44%: 100%|██████████| 572/572 [00:04<00:00, 121.89it/s]\n",
            "[28 / 50]   Val: Loss = 149.58617, Accuracy = 23.25%: : 101it [00:03, 28.66it/s]\n",
            "[29 / 50] Train: Loss = 0.00562, Accuracy = 31.25%: 100%|██████████| 572/572 [00:04<00:00, 123.65it/s]\n",
            "[29 / 50]   Val: Loss = 157.00890, Accuracy = 23.17%: : 101it [00:03, 33.24it/s]\n",
            "[30 / 50] Train: Loss = 0.00512, Accuracy = 31.44%: 100%|██████████| 572/572 [00:04<00:00, 118.70it/s]\n",
            "[30 / 50]   Val: Loss = 152.93128, Accuracy = 23.21%: : 101it [00:03, 30.55it/s]\n",
            "[31 / 50] Train: Loss = 0.00519, Accuracy = 31.40%: 100%|██████████| 572/572 [00:04<00:00, 122.74it/s]\n",
            "[31 / 50]   Val: Loss = 150.67060, Accuracy = 23.72%: : 101it [00:03, 30.89it/s]\n",
            "[32 / 50] Train: Loss = 0.00480, Accuracy = 31.50%: 100%|██████████| 572/572 [00:04<00:00, 119.32it/s]\n",
            "[32 / 50]   Val: Loss = 150.96390, Accuracy = 23.80%: : 101it [00:02, 34.06it/s]\n",
            "[33 / 50] Train: Loss = 0.00442, Accuracy = 31.37%: 100%|██████████| 572/572 [00:04<00:00, 121.22it/s]\n",
            "[33 / 50]   Val: Loss = 160.75211, Accuracy = 23.26%: : 101it [00:03, 30.20it/s]\n",
            "[34 / 50] Train: Loss = 0.00409, Accuracy = 31.59%: 100%|██████████| 572/572 [00:04<00:00, 121.85it/s]\n",
            "[34 / 50]   Val: Loss = 159.73653, Accuracy = 23.64%: : 101it [00:03, 28.31it/s]\n",
            "[35 / 50] Train: Loss = 0.00391, Accuracy = 31.26%: 100%|██████████| 572/572 [00:04<00:00, 115.10it/s]\n",
            "[35 / 50]   Val: Loss = 173.98794, Accuracy = 22.49%: : 101it [00:02, 34.29it/s]\n",
            "[36 / 50] Train: Loss = 0.00391, Accuracy = 31.25%: 100%|██████████| 572/572 [00:04<00:00, 122.70it/s]\n",
            "[36 / 50]   Val: Loss = 162.21572, Accuracy = 23.22%: : 101it [00:03, 28.97it/s]\n",
            "[37 / 50] Train: Loss = 0.00367, Accuracy = 31.46%: 100%|██████████| 572/572 [00:04<00:00, 117.88it/s]\n",
            "[37 / 50]   Val: Loss = 167.61925, Accuracy = 22.90%: : 101it [00:02, 34.46it/s]\n",
            "[38 / 50] Train: Loss = 0.00347, Accuracy = 31.38%: 100%|██████████| 572/572 [00:05<00:00, 113.76it/s]\n",
            "[38 / 50]   Val: Loss = 173.62138, Accuracy = 22.78%: : 101it [00:03, 33.55it/s]\n",
            "[39 / 50] Train: Loss = 0.00334, Accuracy = 31.37%: 100%|██████████| 572/572 [00:04<00:00, 121.58it/s]\n",
            "[39 / 50]   Val: Loss = 182.73597, Accuracy = 22.58%: : 101it [00:03, 31.62it/s]\n",
            "[40 / 50] Train: Loss = 0.00316, Accuracy = 31.52%: 100%|██████████| 572/572 [00:04<00:00, 119.00it/s]\n",
            "[40 / 50]   Val: Loss = 174.52164, Accuracy = 23.17%: : 101it [00:03, 33.60it/s]\n",
            "[41 / 50] Train: Loss = 0.00290, Accuracy = 31.59%: 100%|██████████| 572/572 [00:04<00:00, 118.65it/s]\n",
            "[41 / 50]   Val: Loss = 189.88213, Accuracy = 22.83%: : 101it [00:03, 29.27it/s]\n",
            "[42 / 50] Train: Loss = 0.00284, Accuracy = 31.29%: 100%|██████████| 572/572 [00:04<00:00, 119.06it/s]\n",
            "[42 / 50]   Val: Loss = 177.83322, Accuracy = 23.34%: : 101it [00:02, 34.46it/s]\n",
            "[43 / 50] Train: Loss = 0.00256, Accuracy = 31.28%: 100%|██████████| 572/572 [00:04<00:00, 118.22it/s]\n",
            "[43 / 50]   Val: Loss = 206.94566, Accuracy = 22.14%: : 101it [00:03, 33.62it/s]\n",
            "[44 / 50] Train: Loss = 0.00259, Accuracy = 31.40%: 100%|██████████| 572/572 [00:04<00:00, 117.01it/s]\n",
            "[44 / 50]   Val: Loss = 177.01346, Accuracy = 23.24%: : 101it [00:03, 29.97it/s]\n",
            "[45 / 50] Train: Loss = 0.00247, Accuracy = 31.33%: 100%|██████████| 572/572 [00:04<00:00, 114.59it/s]\n",
            "[45 / 50]   Val: Loss = 179.37908, Accuracy = 23.01%: : 101it [00:02, 33.75it/s]\n",
            "[46 / 50] Train: Loss = 0.00232, Accuracy = 31.65%: 100%|██████████| 572/572 [00:04<00:00, 120.68it/s]\n",
            "[46 / 50]   Val: Loss = 233.61731, Accuracy = 20.87%: : 101it [00:03, 29.46it/s]\n",
            "[47 / 50] Train: Loss = 0.00234, Accuracy = 31.26%: 100%|██████████| 572/572 [00:04<00:00, 119.01it/s]\n",
            "[47 / 50]   Val: Loss = 267.47433, Accuracy = 20.04%: : 101it [00:03, 30.60it/s]\n",
            "[48 / 50] Train: Loss = 0.00230, Accuracy = 31.35%: 100%|██████████| 572/572 [00:04<00:00, 124.00it/s]\n",
            "[48 / 50]   Val: Loss = 196.98729, Accuracy = 22.30%: : 101it [00:03, 33.33it/s]\n",
            "[49 / 50] Train: Loss = 0.00222, Accuracy = 31.38%: 100%|██████████| 572/572 [00:04<00:00, 120.30it/s]\n",
            "[49 / 50]   Val: Loss = 205.57803, Accuracy = 22.09%: : 101it [00:03, 31.61it/s]\n",
            "[50 / 50] Train: Loss = 0.00207, Accuracy = 31.31%: 100%|██████████| 572/572 [00:04<00:00, 122.96it/s]\n",
            "[50 / 50]   Val: Loss = 188.31904, Accuracy = 23.76%: : 101it [00:03, 33.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ],
      "metadata": {
        "id": "MFvfrhRE9wgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_pretrained_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for X_batch_test, y_batch_test in iterate_batches_for_pretrained(test_data, 64):\n",
        "        y_batch_test = torch.cuda.LongTensor(y_batch_test)\n",
        "        logits = model(X_batch_test)\n",
        "        predictions = torch.argmax(logits, dim=2)\n",
        "        correct += torch.sum(predictions == y_batch_test).item()\n",
        "        total += len(predictions.view(-1))\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "v_1f0lj45I7U"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_lstm_accuracy = evaluate_pretrained_model(new_model, X_test, y_test)\n",
        "\n",
        "print(\"Pretrained LSTM test accuracy is {:.2%}\".format(pretrained_lstm_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkA1sAih9xuT",
        "outputId": "8c7b3f03-1e03-4d07-b881-528501efa6e2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained LSTM test accuracy is 24.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохраним лучшую модель"
      ],
      "metadata": {
        "id": "Sqk9Gi9aKor5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(bi_model, \"models/bidirectional_lstm.pth\")"
      ],
      "metadata": {
        "id": "JE1C9aAYKsfZ"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}